{"cells":[{"cell_type":"markdown","metadata":{"id":"0vawE1k-gCea"},"source":["- recent update: 24.10.29\n","- update content:\n","    1. mid-price 생성\n","    2. significant or insignificant 예측 모델 생성\n","    3. significant 예측되는 경우에만 significant increase or decrease인지 예측\n","- target var: mid price return significant change (0 or 1)\n","- Model: XGBoost(significant or insignificant 예측) + LSTM(significant increase or decrease 예측)"]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class TransformerClassifier(nn.Module):\n","    def __init__(self, input_dim, num_classes, num_heads=4, num_transformer_blocks=2, d_model=128, dff=256, dropout_rate=0.1, max_seq_length=100):\n","        super(TransformerClassifier, self).__init__()\n","\n","        # Linear embedding for input features\n","        self.embedding = nn.Linear(input_dim, d_model)\n","\n","        # Positional encoding\n","        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, d_model))\n","\n","        # Transformer encoder blocks\n","        self.transformer_blocks = nn.ModuleList([\n","            nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=dff, dropout=dropout_rate, batch_first=True)\n","            for _ in range(num_transformer_blocks)\n","        ])\n","\n","        # Global average pooling\n","        self.global_avg_pool = nn.AdaptiveAvgPool1d(1)\n","\n","        # Fully connected layer for classification\n","        self.fc = nn.Linear(d_model, num_classes)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x):\n","        # Input shape: (batch_size, seq_length, input_dim)\n","\n","        seq_length = x.size(1)\n","\n","        # Linear embedding and add positional encoding\n","        x = self.embedding(x) + self.positional_encoding[:, :seq_length, :]\n","\n","        # Pass through transformer blocks\n","        for block in self.transformer_blocks:\n","            x = block(x)\n","\n","        # Global average pooling (reduce to (batch_size, d_model))\n","        x = x.transpose(1, 2)  # For pooling (batch_size, d_model, seq_length)\n","        x = self.global_avg_pool(x).squeeze(-1)\n","\n","        # Fully connected layer for classification\n","        x = self.dropout(x)\n","        output = self.fc(x)\n","\n","        return output\n","\n","# Example usage\n","input_dim = 80  # Number of features\n","num_classes = 4  # Number of output classes\n","batch_size = 64\n","seq_length = 100\n","\n","model = TransformerClassifier(input_dim=input_dim, num_classes=num_classes)\n","\n","# Dummy input: (batch_size, seq_length, input_dim)\n","dummy_input = torch.randn(batch_size, seq_length, input_dim)\n","\n","# Forward pass\n","output = model(dummy_input)\n","print(output.shape)  # Expected output: (batch_size, num_classes)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_nF3OMY8FUXd","executionInfo":{"status":"ok","timestamp":1731997661390,"user_tz":-540,"elapsed":308,"user":{"displayName":"Hohyun Kim","userId":"09062334856697746087"}},"outputId":"0ca6016a-f0b5-4db2-a8b3-fbabb597ee69"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 4])\n"]}]},{"cell_type":"code","source":["# Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Training Loop\n","for epoch in range(10):  # Number of epochs\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    # Forward pass\n","    outputs = model(dummy_input)\n","    targets = torch.randint(0, num_classes, (batch_size,))  # Dummy targets\n","    loss = criterion(outputs, targets)\n","\n","    # Backward pass\n","    loss.backward()\n","    optimizer.step()\n","\n","    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AuW9ooH6FWLN","executionInfo":{"status":"ok","timestamp":1731997663406,"user_tz":-540,"elapsed":1728,"user":{"displayName":"Hohyun Kim","userId":"09062334856697746087"}},"outputId":"3389626f-ce8e-4bc3-e9cb-441bc79cd955"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 1.3759\n","Epoch 2, Loss: 1.3653\n","Epoch 3, Loss: 1.3867\n","Epoch 4, Loss: 1.3428\n","Epoch 5, Loss: 1.4148\n","Epoch 6, Loss: 1.3874\n","Epoch 7, Loss: 1.4149\n","Epoch 8, Loss: 1.4384\n","Epoch 9, Loss: 1.4079\n","Epoch 10, Loss: 1.3897\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","class TransformerClassifierNonSequence(nn.Module):\n","    def __init__(self, input_dim, num_classes, num_heads=4, d_model=128, dff=256, dropout_rate=0.1):\n","        super(TransformerClassifierNonSequence, self).__init__()\n","\n","        # Linear embedding to map input_dim to d_model\n","        self.embedding = nn.Linear(input_dim, d_model)\n","\n","        # Single Transformer Encoder Layer\n","        self.self_attention = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n","\n","        # Feedforward Network\n","        self.feed_forward = nn.Sequential(\n","            nn.Linear(d_model, dff),\n","            nn.ReLU(),\n","            nn.Dropout(dropout_rate),\n","            nn.Linear(dff, d_model)\n","        )\n","\n","        # Normalization layers\n","        self.norm1 = nn.LayerNorm(d_model)\n","        self.norm2 = nn.LayerNorm(d_model)\n","\n","        # Fully connected output layer\n","        self.fc = nn.Linear(d_model, num_classes)\n","        self.dropout = nn.Dropout(dropout_rate)\n","\n","    def forward(self, x):\n","        # Input shape: (batch_size, input_dim)\n","\n","        # Add a pseudo-sequence dimension (required for MultiheadAttention)\n","        x = x.unsqueeze(1)  # Shape: (batch_size, 1, input_dim)\n","\n","        # Linear embedding\n","        x = self.embedding(x)  # Shape: (batch_size, 1, d_model)\n","\n","        # Self-attention (requires sequence dimension)\n","        attention_output, _ = self.self_attention(x, x, x)  # Shape: (batch_size, 1, d_model)\n","        x = self.norm1(x + attention_output)\n","\n","        # Feedforward network\n","        ff_output = self.feed_forward(x)\n","        x = self.norm2(x + ff_output)  # Shape: (batch_size, 1, d_model)\n","\n","        # Remove pseudo-sequence dimension\n","        x = x.squeeze(1)  # Shape: (batch_size, d_model)\n","\n","        # Fully connected layer for classification\n","        x = self.dropout(x)\n","        output = self.fc(x)  # Shape: (batch_size, num_classes)\n","\n","        return output\n","\n","# Example usage\n","input_dim = 80  # Number of features\n","num_classes = 4  # Number of output classes\n","batch_size = 64\n","\n","model = TransformerClassifierNonSequence(input_dim=input_dim, num_classes=num_classes)\n","\n","# Dummy input: (batch_size, input_dim)\n","dummy_input = torch.randn(batch_size, input_dim)\n","\n","# Forward pass\n","output = model(dummy_input)\n","print(output.shape)  # Expected output: (batch_size, num_classes)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oHa4_4tRFbPT","executionInfo":{"status":"ok","timestamp":1731997723274,"user_tz":-540,"elapsed":321,"user":{"displayName":"Hohyun Kim","userId":"09062334856697746087"}},"outputId":"c99506aa-f3ab-47b8-eef9-bbbda6edca22"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([64, 4])\n"]}]},{"cell_type":"code","source":["# Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n","\n","# Training Loop\n","for epoch in range(10):  # Number of epochs\n","    model.train()\n","    optimizer.zero_grad()\n","\n","    # Forward pass\n","    outputs = model(dummy_input)\n","    targets = torch.randint(0, num_classes, (batch_size,))  # Dummy targets\n","    loss = criterion(outputs, targets)\n","\n","    # Backward pass\n","    loss.backward()\n","    optimizer.step()\n","\n","    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rpjJ4nSXG-eG","executionInfo":{"status":"ok","timestamp":1731997731362,"user_tz":-540,"elapsed":297,"user":{"displayName":"Hohyun Kim","userId":"09062334856697746087"}},"outputId":"9014a372-3a07-4a71-aa1e-55a69614f722"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1, Loss: 1.4818\n","Epoch 2, Loss: 1.5601\n","Epoch 3, Loss: 1.5841\n","Epoch 4, Loss: 1.4920\n","Epoch 5, Loss: 1.4566\n","Epoch 6, Loss: 1.5573\n","Epoch 7, Loss: 1.4749\n","Epoch 8, Loss: 1.4216\n","Epoch 9, Loss: 1.4681\n","Epoch 10, Loss: 1.4678\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"},"colab":{"provenance":[],"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}