{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN-LSTM v4\n",
    "- timedistributed conv1d를 추가함\n",
    "    - RuntimeError: max_pool1d() Invalid computed output size: 0\n",
    "- 날짜: 2024.03.11\n",
    "- ref: https://medium.com/@mijanr/different-ways-to-combine-cnn-and-lstm-networks-for-time-series-classification-tasks-b03fc37e91b6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Library 불러오기, SEED 설정, CUDA 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps 사용 가능 여부: True\n",
      "mps 지원 환경 여부: True\n"
     ]
    }
   ],
   "source": [
    "# 필요 라이브러리 import\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dataset 관련\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sequence as sq\n",
    "\n",
    "# 성능 평가 관련\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Visualization 관련\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "'''\n",
    "딥러닝 학습을 진행할 때, 가중치를 임의의 값으로 초기화하여 학습을 수행하는 데, \n",
    "실험을 동일하게 진행하기 위해서는 난수를 동일하게 생성해야 한다.\n",
    "Pytorch에서 random seed를 고정하기 위해 manual_seed를 사용한다.\n",
    "'''\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "# 맥북 용\n",
    "#GPU 사용 가능 환경인지 확인 -> mac의 경우 GPU가 아는 MPS를 사용\n",
    "print(f\"mps 사용 가능 여부: {torch.backends.mps.is_available()}\")\n",
    "print(f\"mps 지원 환경 여부: {torch.backends.mps.is_built()}\")\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "# 윈도우 용(Colab)\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#print(f'{device} is available')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 데이터 불러오기 및 전처리 (Binary, Scale, Tensor, train&valid&test split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Data Size: torch.Size([9678, 80, 77]) torch.Size([9678, 1])\n",
      "Train Size: torch.Size([5806, 80, 77]) torch.Size([5806, 1])\n",
      "Valid Size: torch.Size([1936, 80, 77]) torch.Size([1936, 1])\n",
      "Test Size: torch.Size([1936, 80, 77]) torch.Size([1936, 1])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "file_path = '../../data/' # for mac\n",
    "df = pd.read_csv(file_path + 'bitcoin_data_num_rows_gt_5.csv')\n",
    "df = df.iloc[:50000]\n",
    "df['returns_next10m'] = df['returns_next10m'].apply(lambda x: 0 if x <= 0 else 1) # 종속변수 이진분류화\n",
    "df = df.sort_values(by='window_start', ascending=True) # 시간순 정렬\n",
    "\n",
    "# sequence length를 기준으로 sequence 데이터 생성\n",
    "seq_len = 80 # 20, 40, 80, 160, 320\n",
    "X, y = sq.create_sequence(df, seq_len=seq_len)\n",
    "# Tensor화\n",
    "X = torch.FloatTensor(X).to(device)\n",
    "y = torch.FloatTensor(y).to(device)\n",
    "print('Full Data Size:', X.size(), y.size())\n",
    "\n",
    "# split (60% / 20% / 20%)\n",
    "train_split = int((X.size(0)) * 0.6)\n",
    "valid_split = int((X.size(0)) * 0.8)\n",
    "\n",
    "X_train_seq = X[:train_split]\n",
    "X_val_seq = X[train_split:valid_split]\n",
    "X_test_seq = X[valid_split:]\n",
    "y_train_seq = y[:train_split]\n",
    "y_val_seq = y[train_split:valid_split]\n",
    "y_test_seq = y[valid_split:]\n",
    "\n",
    "print('Train Size:', X_train_seq.size(), y_train_seq.size())\n",
    "print('Valid Size:', X_val_seq.size(), y_val_seq.size())\n",
    "print('Test Size:', X_test_seq.size(), y_test_seq.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset과 DataLoader를 이용해 배치 데이터로 만든다.\n",
    "train = torch.utils.data.TensorDataset(X_train_seq, y_train_seq)\n",
    "valid = torch.utils.data.TensorDataset(X_val_seq, y_val_seq)\n",
    "test = torch.utils.data.TensorDataset(X_test_seq, y_test_seq)\n",
    "batch_size = 64 # 32, 64, 128\n",
    "train_loader =  torch.utils.data.DataLoader(dataset=train, batch_size=batch_size, shuffle=False, drop_last=True) # 시계열 데이터기에 shuffle X, 마지막 batch 버림\n",
    "valid_loader = torch.utils.data.DataLoader(dataset=valid, batch_size=batch_size, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 주식 시계열 데이터의 형태: [배치 크기, 시퀀스 길이, 특성 수]\n",
    "# # 여기서 특성 수는 주가, 거래량, 기술적 지표 등 다양한 특성을 포함합니다.\n",
    "\n",
    "# class CNNLSTMModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers):\n",
    "#         super(CNNLSTMModel, self).__init__()\n",
    "        \n",
    "#         # CNN 레이어\n",
    "#         self.cnn = nn.Conv1d(in_channels=input_size[-1], out_channels=64, kernel_size=3)\n",
    "#         self.relu = nn.ReLU()\n",
    "#         self.maxpool = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "#         # LSTM 레이어\n",
    "#         self.lstm = nn.LSTM(input_size=64, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "#         # Fully Connected 레이어\n",
    "#         self.fc = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # CNN 레이어 적용\n",
    "#         x = self.cnn(x)\n",
    "#         x = self.relu(x)\n",
    "#         x = self.maxpool(x)\n",
    "        \n",
    "#         # LSTM 레이어 적용\n",
    "#         lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "#         # Fully Connected 레이어에 입력\n",
    "#         out = self.fc(lstm_out[:, -1, :])\n",
    "        \n",
    "#         return out\n",
    "\n",
    "# num_features = X.size(2)\n",
    "\n",
    "# # 모델 인스턴스화\n",
    "# input_size = [batch_size, seq_len, num_features]  # 예: [32, 20, 5]\n",
    "# hidden_size = 64\n",
    "# num_layers = 2\n",
    "\n",
    "# model = CNNLSTMModel(input_size, hidden_size, num_layers).to(device)\n",
    "\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CNNLSTMModel(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "#         super(CNNLSTMModel, self).__init__()\n",
    "        \n",
    "#         # CNN 레이어\n",
    "#         # in_channels=input_size[-1]이면 안됨. 즉 feature_dims면 안되고 sequence_length(=20, 40, 80...) 여야 함.\n",
    "#         '''\n",
    "#         in_channels = 일반적인 이미지와 같은 2D 데이터를 다룰 때는 특성 맵(channel)을 채널로 인식함.\n",
    "#         그러나 주식 시계열 데이터와 같은 1D 데이터의 경우 시퀀스 길이에 해당하는 차원이 채널로 간주됨.\n",
    "#         이에 따라 'in_channels'에는 시퀀스 길이를 입력해야 함.\n",
    "#         즉, 주식 시게열 데이터에서는 'in_channels'에는 시퀀스의 길이가 들어가야 올바르게 수행됨.\n",
    "#         '''\n",
    "#         self.cnn = nn.Sequential(\n",
    "#             nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "#             nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "#         )\n",
    "        \n",
    "#         # LSTM 레이어\n",
    "#         self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, num_classes) # Fully Connected 레이어\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # CNN 레이어 적용 (cnn takes input of shape (batch_size, channels, seq_len))\n",
    "#         x = x.permute(0, 2, 1)\n",
    "#         out = self.cnn(x)\n",
    "        \n",
    "#         # LSTM 레이어 적용\n",
    "#         '''\n",
    "#         LSTM 레이어에 입력을 전달하고, LSTM의 출력과 은닉 상태를 받는 부분\n",
    "#         x.permute(0, 2, 1): 입력텐서 x의 차원을 변경. 일반적으로 LSTM 레이어는 시간 단계(seq_len)를 두 번쨰 차원으로 받지만,\n",
    "#         Conv1d 레이어의 출력은 시간 단계가 세번째 차원에 위치함. 따라서 permute를 통해 차원을 변경하여 LSTM 레이어에 올바른 형태의 입력을 제공\n",
    "#         여기서 0번째 차원은 배치 크기(batch_size)를 나타내며, 1번째 차원은 특성 수(num_features)를 나타냄. 마지막(2번째) 차원은 시간 단계(seq_len)를 나타냄\n",
    "#         self.lstm(x.permute(0, 2, 1)): 변경된 입력을 LSTM 레이어에 전달함. LSTM 입력으로 3D 텐서를 받으며,\n",
    "#         이 텐서는 배치 크기(batch_size), 시간 단계(seq_len),. 특성 수(num_features)의 형태를 가짐\n",
    "#         lstm_out, _: LSTM 레이어의 출력과 은닉 상태를 받음. 여기서 은닉 상태는 사용하지 않기 때문에 '_'로 무시. lstm_out은 LSTM 레이어의 출력으로, 각 시간 단계에\n",
    "#         해당하는 출력을 포함하는 3D 텐서임.\n",
    "#         '''\n",
    "#         # lstm takes input of shape (batch_size, seq_len, input_size)\n",
    "#         out = out.permute(0, 2, 1)\n",
    "#         out, _ = self.lstm(out)\n",
    "        \n",
    "#         # Fully Connected 레이어에 입력\n",
    "#         '''\n",
    "#         lstm_out[:, -1, :]: LSTM 레이어의 출력에서 마지막 시간 단계의 출력만 선택. 이는 시퀀스 예측을 위해 마지막 시간 단계의 정보만을 사용하고자 하는 것\n",
    "#         따라서 [:, -1, :]는 모든 배치와 모든 특성을 유지하면서 마지막 시간 단계의 출력을 선택함\n",
    "#         self.fc(lstm_out[:, -1, :]): 선택된 마지막 시간 단계의 출력을 Fully Connected(FC) 레이어에 입력함. FC 레이어는 입력된 LSTM 출력을 받아서 최종\n",
    "#         예측을 수행하는 역할을 함. 출력 크기는 1이며, 이는 주어진 입력에 대한 예측된 결과를 나타냄.\n",
    "#         '''\n",
    "#         out = self.fc(out[:, -1, :]) \n",
    "#         return out\n",
    "\n",
    "# #model = CNNLSTMModel(input_size=77, hidden_size=64, num_layers=2, num_classes=1)\n",
    "# model = CNNLSTMModel(input_size=77, hidden_size=128, num_layers=2, num_classes=1)\n",
    "# model.to(device) # GPU 사용 시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNNLSTMModel(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv1d(77, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (1): ReLU()\n",
       "    (2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "    (4): ReLU()\n",
       "    (5): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (lstm): LSTM(128, 128, num_layers=2, batch_first=True)\n",
       "  (fc): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_size, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=128, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 입력 x의 차원: (batch_size, seq_len, input_size)\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        x = x.permute(0, 2, 1) # 차원 변경: (batch_size, input_size, seq_len)\n",
    "        \n",
    "        # CNN 연산을 시간 단계별로 적용\n",
    "        cnn_out = []\n",
    "        for t in range(seq_len):\n",
    "            # 각 시간 단계별로 데이터 슬라이싱\n",
    "            xt = x[:, :, t:t+1] # 차원 유지를 위해 슬라이싱 사용\n",
    "            # CNN 레이어 적용\n",
    "            out = self.cnn(xt)\n",
    "            # 결과 저장\n",
    "            cnn_out.append(out)\n",
    "        # 시간 단계를 따라 결과를 합침\n",
    "        out = torch.cat(cnn_out, dim=2) # (batch_size, 128, new_seq_len)\n",
    "        \n",
    "        # LSTM 레이어 적용을 위해 차원 변경\n",
    "        out = out.permute(0, 2, 1) # (batch_size, new_seq_len, 128)\n",
    "        out, _ = self.lstm(out)\n",
    "        \n",
    "        # Fully Connected 레이어 적용\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "model = CNNLSTMModel(input_size=77, hidden_size=128, num_layers=2, num_classes=1)\n",
    "model.to(device) # GPU 사용 시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 모델학습1: train 데이터만 가지고 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "max_pool1d() Invalid computed output size: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m batch_features, batch_targets \u001b[38;5;241m=\u001b[39m batch_features\u001b[38;5;241m.\u001b[39mto(device), batch_targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# 순전파: 모델에 데이터를 전달하여 출력을 계산\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# 손실 계산\u001b[39;00m\n\u001b[1;32m     29\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39msqueeze(), batch_targets)  \u001b[38;5;66;03m# outputs.squeeze()는 차원 축소가 필요한 경우 사용\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m, in \u001b[0;36mCNNLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m xt \u001b[38;5;241m=\u001b[39m x[:, :, t:t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# 차원 유지를 위해 슬라이싱 사용\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# CNN 레이어 적용\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n\u001b[1;32m     33\u001b[0m cnn_out\u001b[38;5;241m.\u001b[39mappend(out)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/pooling.py:92\u001b[0m, in \u001b[0;36mMaxPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/_jit_internal.py:488\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/functional.py:705\u001b[0m, in \u001b[0;36m_max_pool1d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: max_pool1d() Invalid computed output size: 0"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 가정: train_loader는 이미 정의되어 있으며, 학습 데이터를 제공합니다.\n",
    "# 예: train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 정의\n",
    "criterion = nn.BCEWithLogitsLoss() # 이진 분류 문제의 경우\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 파라미터 설정\n",
    "num_epochs = 20\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_features, batch_targets in train_loader:\n",
    "        # 배치 데이터를 GPU로 전송 (GPU 사용시)\n",
    "        batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "        \n",
    "        # 순전파: 모델에 데이터를 전달하여 출력을 계산\n",
    "        outputs = model(batch_features)\n",
    "        \n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs.squeeze(), batch_targets)  # outputs.squeeze()는 차원 축소가 필요한 경우 사용\n",
    "        \n",
    "        # 역전파: 손실의 그라디언트를 계산\n",
    "        optimizer.zero_grad()  # 그라디언트 초기화\n",
    "        loss.backward()\n",
    "        \n",
    "        # 파라미터 업데이트\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # 에폭마다 평균 손실 출력\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "max_pool1d() Invalid computed output size: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# 모델에 대한 순전파 및 손실 계산\u001b[39;00m\n\u001b[1;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_targets)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# 역전파 및 최적화\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[13], line 31\u001b[0m, in \u001b[0;36mCNNLSTMModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m xt \u001b[38;5;241m=\u001b[39m x[:, :, t:t\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m# 차원 유지를 위해 슬라이싱 사용\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# CNN 레이어 적용\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 결과 저장\u001b[39;00m\n\u001b[1;32m     33\u001b[0m cnn_out\u001b[38;5;241m.\u001b[39mappend(out)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/modules/pooling.py:92\u001b[0m, in \u001b[0;36mMaxPool1d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[0;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/_jit_internal.py:488\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/nn/functional.py:705\u001b[0m, in \u001b[0;36m_max_pool1d\u001b[0;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[0;32m--> 705\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool1d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: max_pool1d() Invalid computed output size: 0"
     ]
    }
   ],
   "source": [
    "# 학습1: train data만 가지고 학습 -> 과적합 이빠이~ -> 15 epochs만 돌리자..\n",
    "\n",
    "# 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_features, batch_targets in train_loader:\n",
    "        # 배치를 GPU로 전송\n",
    "        batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "        \n",
    "        # 모델에 대한 순전파 및 손실 계산\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        \n",
    "        # 역전파 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # 에폭마다 손실 출력\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {total_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 모델학습2: train, valid를 이용한 과적합 방지되는 epoch 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 0.005403854099175784, Valid Loss: 0.005724508099812122\n",
      "Epoch 2/100, Train Loss: 0.005381310556906485, Valid Loss: 0.005724905905398456\n",
      "Epoch 3/100, Train Loss: 0.005379725079515085, Valid Loss: 0.005724533992119072\n",
      "Epoch 4/100, Train Loss: 0.005371083206773995, Valid Loss: 0.00571058314940161\n",
      "Epoch 5/100, Train Loss: 0.005386581050910253, Valid Loss: 0.005724036896770651\n",
      "Epoch 6/100, Train Loss: 0.005400053630645547, Valid Loss: 0.0057244169995311865\n",
      "Epoch 7/100, Train Loss: 0.005376391798062773, Valid Loss: 0.005724205520034821\n",
      "Epoch 8/100, Train Loss: 0.005374778571557555, Valid Loss: 0.0057227764863613225\n",
      "Epoch 9/100, Train Loss: 0.005372058716963539, Valid Loss: 0.005710055482042722\n",
      "Epoch 10/100, Train Loss: 0.005374884650578303, Valid Loss: 0.005696977527180979\n",
      "Epoch 11/100, Train Loss: 0.0052592318775815635, Valid Loss: 0.005397521020952335\n",
      "Epoch 12/100, Train Loss: 0.00474465089935785, Valid Loss: 0.004954361472248046\n",
      "Epoch 13/100, Train Loss: 0.004361966268752634, Valid Loss: 0.004163598995809713\n",
      "Epoch 14/100, Train Loss: 0.0039423224363826365, Valid Loss: 0.004107648583729405\n",
      "Epoch 15/100, Train Loss: 0.003756530349352341, Valid Loss: 0.004023863141201745\n",
      "Epoch 16/100, Train Loss: 0.003593527540292487, Valid Loss: 0.003967754989246692\n",
      "Epoch 17/100, Train Loss: 0.00344353257234122, Valid Loss: 0.004118650135668841\n",
      "Epoch 18/100, Train Loss: 0.0033214583433458732, Valid Loss: 0.004459020081322548\n",
      "Epoch 19/100, Train Loss: 0.003221162867225781, Valid Loss: 0.003950740757190491\n",
      "Epoch 20/100, Train Loss: 0.003160599538305074, Valid Loss: 0.004444531234335308\n",
      "Epoch 21/100, Train Loss: 0.0030513356522860052, Valid Loss: 0.004477233356557602\n",
      "Epoch 22/100, Train Loss: 0.002952181998816105, Valid Loss: 0.004715805827093519\n",
      "Epoch 23/100, Train Loss: 0.0028284622694433373, Valid Loss: 0.005086577335788199\n",
      "Epoch 24/100, Train Loss: 0.002696277422922215, Valid Loss: 0.005229833389609313\n",
      "Epoch 25/100, Train Loss: 0.0027319630450763827, Valid Loss: 0.005114480144967718\n",
      "Epoch 26/100, Train Loss: 0.0027019864874094713, Valid Loss: 0.00521406537491428\n",
      "Epoch 27/100, Train Loss: 0.002721931383686151, Valid Loss: 0.004775291231791835\n",
      "Epoch 28/100, Train Loss: 0.0024872496595803187, Valid Loss: 0.005763431272107707\n",
      "Epoch 29/100, Train Loss: 0.0022696682473030577, Valid Loss: 0.006107485737682374\n",
      "Epoch 30/100, Train Loss: 0.0022290135389549997, Valid Loss: 0.006032680854016591\n",
      "Epoch 31/100, Train Loss: 0.0022531954434938033, Valid Loss: 0.0058120850412067306\n",
      "Epoch 32/100, Train Loss: 0.0023304163675615386, Valid Loss: 0.0060436870856595435\n",
      "Epoch 33/100, Train Loss: 0.00222752494670135, Valid Loss: 0.005661288153164643\n",
      "Epoch 34/100, Train Loss: 0.0019781071971171566, Valid Loss: 0.00624116807252415\n",
      "Epoch 35/100, Train Loss: 0.0018819796817798758, Valid Loss: 0.0059244383265041126\n",
      "Epoch 36/100, Train Loss: 0.0017939941759330012, Valid Loss: 0.005615155928391071\n",
      "Epoch 37/100, Train Loss: 0.0016689645283660764, Valid Loss: 0.005519091898252156\n",
      "Epoch 38/100, Train Loss: 0.0017544660021724513, Valid Loss: 0.005782965185844208\n",
      "Epoch 39/100, Train Loss: 0.0016125727430771352, Valid Loss: 0.006053795160587169\n",
      "Epoch 40/100, Train Loss: 0.0016313548791092344, Valid Loss: 0.005867464313945495\n",
      "Epoch 41/100, Train Loss: 0.001521734303922806, Valid Loss: 0.006356453877096334\n",
      "Epoch 42/100, Train Loss: 0.0014382236679843323, Valid Loss: 0.006284596460047832\n",
      "Epoch 43/100, Train Loss: 0.0013436819463608965, Valid Loss: 0.006085857287172444\n",
      "Epoch 44/100, Train Loss: 0.0013644591517954336, Valid Loss: 0.006537861029101797\n",
      "Epoch 45/100, Train Loss: 0.0013352436806892388, Valid Loss: 0.007177630377825627\n",
      "Epoch 46/100, Train Loss: 0.00129618115345856, Valid Loss: 0.005954658073827255\n",
      "Epoch 47/100, Train Loss: 0.0012819893706924868, Valid Loss: 0.006984390195243615\n",
      "Epoch 48/100, Train Loss: 0.001376184964340143, Valid Loss: 0.00801418949503544\n",
      "Epoch 49/100, Train Loss: 0.0017518120536486363, Valid Loss: 0.005816246913976906\n",
      "Epoch 50/100, Train Loss: 0.0015225576245185223, Valid Loss: 0.006154511123895645\n",
      "Epoch 51/100, Train Loss: 0.0014860474844913913, Valid Loss: 0.006283462647933605\n",
      "Epoch 52/100, Train Loss: 0.0014765831958496115, Valid Loss: 0.006499119395436334\n",
      "Epoch 53/100, Train Loss: 0.0014099265803889985, Valid Loss: 0.00714517283168706\n",
      "Epoch 54/100, Train Loss: 0.0012613587874484153, Valid Loss: 0.006942712663372686\n",
      "Epoch 55/100, Train Loss: 0.0011640135120232168, Valid Loss: 0.007489852114649843\n",
      "Epoch 56/100, Train Loss: 0.0011650475660865774, Valid Loss: 0.007738054260488384\n",
      "Epoch 57/100, Train Loss: 0.0011535428109592583, Valid Loss: 0.0075818444640675855\n",
      "Epoch 58/100, Train Loss: 0.0012240307402382872, Valid Loss: 0.008128454647034654\n",
      "Epoch 59/100, Train Loss: 0.0013286258510108286, Valid Loss: 0.007889903035045656\n",
      "Epoch 60/100, Train Loss: 0.001487806407959963, Valid Loss: 0.007346189403829495\n",
      "Epoch 61/100, Train Loss: 0.0011700218372737339, Valid Loss: 0.007282546513583049\n",
      "Epoch 62/100, Train Loss: 0.0011458959647967944, Valid Loss: 0.007693613579204259\n",
      "Epoch 63/100, Train Loss: 0.001092164961366439, Valid Loss: 0.007777223502062569\n",
      "Epoch 64/100, Train Loss: 0.001045903581962287, Valid Loss: 0.00828082309958856\n",
      "Epoch 65/100, Train Loss: 0.0009854204617145551, Valid Loss: 0.008164716561224835\n",
      "Epoch 66/100, Train Loss: 0.001012219917645105, Valid Loss: 0.008563487274833947\n",
      "Epoch 67/100, Train Loss: 0.0010367538632399618, Valid Loss: 0.008358515737470517\n",
      "Epoch 68/100, Train Loss: 0.0008751047297778353, Valid Loss: 0.009015114870199487\n",
      "Epoch 69/100, Train Loss: 0.0008621063173879765, Valid Loss: 0.009239940788627657\n",
      "Epoch 70/100, Train Loss: 0.0008988301904921466, Valid Loss: 0.008388485359258888\n",
      "Epoch 71/100, Train Loss: 0.0007828362313360027, Valid Loss: 0.008728248706041289\n",
      "Epoch 72/100, Train Loss: 0.0006460870035144119, Valid Loss: 0.009229315337070748\n",
      "Epoch 73/100, Train Loss: 0.0006746839282150684, Valid Loss: 0.009508138630262092\n",
      "Epoch 74/100, Train Loss: 0.0006053860946049926, Valid Loss: 0.010137995197014376\n",
      "Epoch 75/100, Train Loss: 0.0006298963930117768, Valid Loss: 0.01068401071897223\n",
      "Epoch 76/100, Train Loss: 0.0005427843070923615, Valid Loss: 0.01095013031043297\n",
      "Epoch 77/100, Train Loss: 0.0005728629574675627, Valid Loss: 0.01068088932593992\n",
      "Epoch 78/100, Train Loss: 0.0005754972278519687, Valid Loss: 0.01021599985104947\n",
      "Epoch 79/100, Train Loss: 0.000592712839031377, Valid Loss: 0.010102248234936028\n",
      "Epoch 80/100, Train Loss: 0.0004941876447336736, Valid Loss: 0.010578416965224526\n",
      "Epoch 81/100, Train Loss: 0.00037987905391179155, Valid Loss: 0.01073681378413823\n",
      "Epoch 82/100, Train Loss: 0.0003128443269697963, Valid Loss: 0.011280770513637007\n",
      "Epoch 83/100, Train Loss: 0.00029862793234347444, Valid Loss: 0.011726735144360992\n",
      "Epoch 84/100, Train Loss: 0.00024108616079994912, Valid Loss: 0.01234732749910394\n",
      "Epoch 85/100, Train Loss: 0.0001870324593808924, Valid Loss: 0.012891020410317035\n",
      "Epoch 86/100, Train Loss: 0.00020403395361822478, Valid Loss: 0.01312415960652769\n",
      "Epoch 87/100, Train Loss: 0.00019692779732646715, Valid Loss: 0.013776965316169518\n",
      "Epoch 88/100, Train Loss: 0.00023957554942830728, Valid Loss: 0.013457699286297333\n",
      "Epoch 89/100, Train Loss: 0.0002676361795294102, Valid Loss: 0.013593043239156076\n",
      "Epoch 90/100, Train Loss: 0.0003385838863699396, Valid Loss: 0.012715235752753976\n",
      "Epoch 91/100, Train Loss: 0.00043070228305686434, Valid Loss: 0.012541935402007142\n",
      "Epoch 92/100, Train Loss: 0.0004481069675139516, Valid Loss: 0.011136063582394735\n",
      "Epoch 93/100, Train Loss: 0.00029209656403986293, Valid Loss: 0.011677403218490033\n",
      "Epoch 94/100, Train Loss: 0.00026268786735495197, Valid Loss: 0.013105045111218759\n",
      "Epoch 95/100, Train Loss: 0.00019865607491271236, Valid Loss: 0.01319446590197973\n",
      "Epoch 96/100, Train Loss: 0.00014437833532470924, Valid Loss: 0.014506213982735784\n",
      "Epoch 97/100, Train Loss: 0.00012163440639048377, Valid Loss: 0.014408693610390357\n",
      "Epoch 98/100, Train Loss: 7.53824078010373e-05, Valid Loss: 0.01521370418308195\n",
      "Epoch 99/100, Train Loss: 6.188642089905604e-05, Valid Loss: 0.01641606367077709\n",
      "Epoch 100/100, Train Loss: 9.805757831172556e-05, Valid Loss: 0.016412483020262283\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학습2: valid를 이용한 과적합 방지 epoch 찾기\n",
    "\n",
    "# 학습과 검증 손실을 저장할 리스트 초기화\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "\n",
    "# # 손실 함수와 옵티마이저 정의\n",
    "criterion = nn.BCEWithLogitsLoss() # 시그모이드 활성화 함수가 내장되어 있음. 모델의 마지막 레이어에서 시그모이드 함수 별도 적용할 필요X\n",
    "#criterion = nn.BCELoss() # 모델 출력이 시그모이드 활성화 함수를 거쳐 확률로 변환된 후의 값을 입력으로 받음. 입력 값은 0과 1사이의 확률 값.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# 검증 데이터에 대한 모델 성능 평가 함수 정의\n",
    "def evaluate(model, criterion, dataloader):\n",
    "    model.eval()  # 모델을 평가 모드로 설정\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_targets in dataloader:\n",
    "            # 배치를 GPU로 전송\n",
    "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "            \n",
    "            # 모델에 대한 순전파 및 손실 계산\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_targets)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader.dataset)  # 평균 손실 반환\n",
    "\n",
    "# 학습 루프\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_features, batch_targets in train_loader:\n",
    "        # 배치를 GPU로 전송\n",
    "        batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "        \n",
    "        # 모델에 대한 순전파 및 손실 계산\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_features)\n",
    "        loss = criterion(outputs, batch_targets)\n",
    "        \n",
    "        # 역전파 및 최적화\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # 에폭마다 학습 손실 기록\n",
    "    train_loss = total_loss / len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # 검증 데이터에 대한 손실 계산 및 기록\n",
    "    valid_loss = evaluate(model, criterion, valid_loader)\n",
    "    valid_losses.append(valid_loss)\n",
    "    \n",
    "    # 에폭마다 손실 출력\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss}, Valid Loss: {valid_loss}')\n",
    "\n",
    "# 손실 함수 시각화\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(valid_losses, label='Valid Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Imbalance: Counter({1.0: 984, 0.0: 952})\n",
      "Accuracy: 0.7149\n",
      "Precision: 0.6782\n",
      "Recall: 0.8354\n",
      "F1 Score: 0.7486\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "model.eval()  # 모델을 평가 모드로 설정\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for x_batch, labels in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        outputs = model(x_batch)\n",
    "\n",
    "        # 로그 오즈를 확률로 변환\n",
    "        probs = torch.sigmoid(outputs).squeeze()\n",
    "\n",
    "        # 확률을 기준으로 0.5 이상이면 1, 미만이면 0으로 예측\n",
    "        preds = torch.round(probs).cpu().numpy()\n",
    "        y_true.extend(labels.squeeze().cpu().numpy())\n",
    "        y_pred.extend(preds)\n",
    "\n",
    "# 성능 지표 계산\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# 결과 출력\n",
    "print(f'Data Imbalance: {Counter(y_true)}')\n",
    "print(f'Accuracy: {accuracy.round(4)}')\n",
    "print(f'Precision: {precision.round(4)}')\n",
    "print(f'Recall: {recall.round(4)}')\n",
    "print(f'F1 Score: {f1.round(4)}')\n",
    "\n",
    "# 혼동 행렬 출력\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title(f'Confusion Matrix Sequence Length({seq_len})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 모델학습3: Optuna + CV 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Data Size: torch.Size([7669, 20, 77]) torch.Size([7669, 1])\n",
      "Train Size: torch.Size([5368, 20, 77]) torch.Size([5368, 1])\n",
      "Test Size: torch.Size([2301, 20, 77]) torch.Size([2301, 1])\n"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기 / Optuna 용 -> valid 제거\n",
    "file_path = '../../data/' # for mac\n",
    "df = pd.read_csv(file_path + 'bitcoin_data_num_rows_gt_5.csv')\n",
    "df = df.iloc[:20000]\n",
    "df['returns_next10m'] = df['returns_next10m'].apply(lambda x: 0 if x <= 0 else 1) # 종속변수 이진분류화\n",
    "df = df.sort_values(by='window_start', ascending=True) # 시간순 정렬\n",
    "\n",
    "# sequence length를 기준으로 sequence 데이터 생성\n",
    "seq_len = 20 # 20, 40, 80, 160, 320\n",
    "X, y = sq.create_sequence(df, seq_len=seq_len)\n",
    "# Tensor화\n",
    "X = torch.FloatTensor(X).to(device)\n",
    "y = torch.FloatTensor(y).to(device)\n",
    "print('Full Data Size:', X.size(), y.size())\n",
    "\n",
    "# split (70% / 30%)\n",
    "split = int((X.size(0)) * 0.7)\n",
    "\n",
    "X_train_seq = X[:split]\n",
    "X_test_seq = X[split:]\n",
    "y_train_seq = y[:split]\n",
    "y_test_seq = y[split:]\n",
    "\n",
    "print('Train Size:', X_train_seq.size(), y_train_seq.size())\n",
    "print('Test Size:', X_test_seq.size(), y_test_seq.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-28 15:28:59,176] A new study created in memory with name: no-name-d6822665-db7f-4e2f-9eb1-f49e92bf51f4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-28 15:29:46,390] Trial 0 finished with value: 0.7014174699783325 and parameters: {'hidden_size': 10, 'num_layers': 2, 'lr': 0.1}. Best is trial 0 with value: 0.7014174699783325.\n",
      "[I 2024-02-28 15:30:57,182] Trial 1 finished with value: 0.7083777785301208 and parameters: {'hidden_size': 100, 'num_layers': 4, 'lr': 0.1}. Best is trial 0 with value: 0.7014174699783325.\n",
      "[I 2024-02-28 15:31:54,308] Trial 2 finished with value: 0.7014065384864807 and parameters: {'hidden_size': 25, 'num_layers': 3, 'lr': 0.1}. Best is trial 2 with value: 0.7014065384864807.\n",
      "[I 2024-02-28 15:33:07,379] Trial 3 finished with value: 0.7007116734981537 and parameters: {'hidden_size': 95, 'num_layers': 4, 'lr': 0.01}. Best is trial 3 with value: 0.7007116734981537.\n",
      "[I 2024-02-28 15:34:08,575] Trial 4 finished with value: 0.7073028862476349 and parameters: {'hidden_size': 50, 'num_layers': 3, 'lr': 0.1}. Best is trial 3 with value: 0.7007116734981537.\n",
      "[I 2024-02-28 15:35:36,268] Trial 5 finished with value: 0.8319100975990296 and parameters: {'hidden_size': 95, 'num_layers': 5, 'lr': 0.001}. Best is trial 3 with value: 0.7007116734981537.\n",
      "[I 2024-02-28 15:36:31,126] Trial 6 finished with value: 0.6982091009616852 and parameters: {'hidden_size': 95, 'num_layers': 2, 'lr': 0.0001}. Best is trial 6 with value: 0.6982091009616852.\n",
      "[I 2024-02-28 15:37:25,542] Trial 7 finished with value: 0.7056076526641846 and parameters: {'hidden_size': 85, 'num_layers': 2, 'lr': 0.01}. Best is trial 6 with value: 0.6982091009616852.\n",
      "[I 2024-02-28 15:38:55,724] Trial 8 finished with value: 0.7596176505088806 and parameters: {'hidden_size': 85, 'num_layers': 5, 'lr': 0.001}. Best is trial 6 with value: 0.6982091009616852.\n",
      "[I 2024-02-28 15:39:56,962] Trial 9 finished with value: 0.7099454998970032 and parameters: {'hidden_size': 100, 'num_layers': 2, 'lr': 0.1}. Best is trial 6 with value: 0.6982091009616852.\n",
      "[I 2024-02-28 15:41:03,436] Trial 10 finished with value: 0.6977709770202637 and parameters: {'hidden_size': 60, 'num_layers': 3, 'lr': 0.0001}. Best is trial 10 with value: 0.6977709770202637.\n",
      "[I 2024-02-28 15:42:14,503] Trial 11 finished with value: 0.699049311876297 and parameters: {'hidden_size': 65, 'num_layers': 3, 'lr': 0.0001}. Best is trial 10 with value: 0.6977709770202637.\n",
      "[I 2024-02-28 15:43:25,045] Trial 12 finished with value: 0.6980584979057312 and parameters: {'hidden_size': 50, 'num_layers': 2, 'lr': 0.0001}. Best is trial 10 with value: 0.6977709770202637.\n",
      "[I 2024-02-28 15:44:40,692] Trial 13 finished with value: 0.6969872713088989 and parameters: {'hidden_size': 50, 'num_layers': 3, 'lr': 0.0001}. Best is trial 13 with value: 0.6969872713088989.\n",
      "[I 2024-02-28 15:46:15,717] Trial 14 finished with value: 0.6972411453723908 and parameters: {'hidden_size': 65, 'num_layers': 4, 'lr': 0.0001}. Best is trial 13 with value: 0.6969872713088989.\n",
      "[I 2024-02-28 15:47:42,136] Trial 15 finished with value: 0.6982307016849518 and parameters: {'hidden_size': 35, 'num_layers': 4, 'lr': 0.0001}. Best is trial 13 with value: 0.6969872713088989.\n",
      "[I 2024-02-28 15:48:49,379] Trial 16 finished with value: 0.6980492770671844 and parameters: {'hidden_size': 70, 'num_layers': 4, 'lr': 0.0001}. Best is trial 13 with value: 0.6969872713088989.\n",
      "[I 2024-02-28 15:49:54,326] Trial 17 finished with value: 0.696724820137024 and parameters: {'hidden_size': 40, 'num_layers': 5, 'lr': 0.0001}. Best is trial 17 with value: 0.696724820137024.\n",
      "[I 2024-02-28 15:50:58,129] Trial 18 finished with value: 0.69744313955307 and parameters: {'hidden_size': 40, 'num_layers': 5, 'lr': 0.0001}. Best is trial 17 with value: 0.696724820137024.\n",
      "[I 2024-02-28 15:52:02,011] Trial 19 finished with value: 0.9157582461833954 and parameters: {'hidden_size': 20, 'num_layers': 5, 'lr': 0.001}. Best is trial 17 with value: 0.696724820137024.\n",
      "[I 2024-02-28 15:52:51,718] Trial 20 finished with value: 0.7588746547698975 and parameters: {'hidden_size': 40, 'num_layers': 3, 'lr': 0.01}. Best is trial 17 with value: 0.696724820137024.\n",
      "[I 2024-02-28 15:53:49,845] Trial 21 finished with value: 0.6967857778072357 and parameters: {'hidden_size': 75, 'num_layers': 4, 'lr': 0.0001}. Best is trial 17 with value: 0.696724820137024.\n",
      "[I 2024-02-28 15:54:57,520] Trial 22 finished with value: 0.6989129304885864 and parameters: {'hidden_size': 75, 'num_layers': 5, 'lr': 0.0001}. Best is trial 17 with value: 0.696724820137024.\n",
      "[I 2024-02-28 15:55:55,700] Trial 23 finished with value: 0.6980346381664276 and parameters: {'hidden_size': 50, 'num_layers': 4, 'lr': 0.0001}. Best is trial 17 with value: 0.696724820137024.\n",
      "[I 2024-02-28 15:56:44,987] Trial 24 finished with value: 0.6970070362091064 and parameters: {'hidden_size': 30, 'num_layers': 3, 'lr': 0.0001}. Best is trial 17 with value: 0.696724820137024.\n",
      "[I 2024-02-28 15:57:50,634] Trial 25 finished with value: 0.6973746359348297 and parameters: {'hidden_size': 55, 'num_layers': 5, 'lr': 0.0001}. Best is trial 17 with value: 0.696724820137024.\n",
      "[I 2024-02-28 15:58:51,645] Trial 26 finished with value: 0.6990817427635193 and parameters: {'hidden_size': 80, 'num_layers': 4, 'lr': 0.0001}. Best is trial 17 with value: 0.696724820137024.\n",
      "[I 2024-02-28 15:59:41,826] Trial 27 finished with value: 0.6984017729759217 and parameters: {'hidden_size': 40, 'num_layers': 3, 'lr': 0.0001}. Best is trial 17 with value: 0.696724820137024.\n",
      "[I 2024-02-28 16:00:37,904] Trial 28 finished with value: 0.7303711712360382 and parameters: {'hidden_size': 15, 'num_layers': 4, 'lr': 0.001}. Best is trial 17 with value: 0.696724820137024.\n",
      "[I 2024-02-28 16:01:38,410] Trial 29 finished with value: 0.7251190185546875 and parameters: {'hidden_size': 10, 'num_layers': 5, 'lr': 0.01}. Best is trial 17 with value: 0.696724820137024.\n",
      "[I 2024-02-28 16:02:32,534] Trial 30 finished with value: 0.6973938941955566 and parameters: {'hidden_size': 45, 'num_layers': 4, 'lr': 0.0001}. Best is trial 17 with value: 0.696724820137024.\n",
      "[I 2024-02-28 16:03:19,746] Trial 31 finished with value: 0.6995476722717285 and parameters: {'hidden_size': 30, 'num_layers': 3, 'lr': 0.0001}. Best is trial 17 with value: 0.696724820137024.\n",
      "[I 2024-02-28 16:04:10,372] Trial 32 finished with value: 0.687905079126358 and parameters: {'hidden_size': 5, 'num_layers': 3, 'lr': 0.0001}. Best is trial 32 with value: 0.687905079126358.\n",
      "[I 2024-02-28 16:05:02,231] Trial 33 finished with value: 0.7219882071018219 and parameters: {'hidden_size': 5, 'num_layers': 3, 'lr': 0.0001}. Best is trial 32 with value: 0.687905079126358.\n",
      "[I 2024-02-28 16:05:56,004] Trial 34 finished with value: 0.70503551363945 and parameters: {'hidden_size': 25, 'num_layers': 3, 'lr': 0.1}. Best is trial 32 with value: 0.687905079126358.\n",
      "[I 2024-02-28 16:06:49,951] Trial 35 finished with value: 0.6965234041213989 and parameters: {'hidden_size': 60, 'num_layers': 3, 'lr': 0.0001}. Best is trial 32 with value: 0.687905079126358.\n",
      "[W 2024-02-28 16:07:49,299] Trial 36 failed with parameters: {'hidden_size': 60, 'num_layers': 4, 'lr': 0.1} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/lib/python3.8/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/var/folders/yd/1_xwcyjj6z58p2vptxk1dwvm0000gn/T/ipykernel_27359/1271495075.py\", line 47, in objective\n",
      "    optimizer.step()\n",
      "  File \"/opt/homebrew/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 373, in wrapper\n",
      "    out = func(*args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.8/site-packages/torch/optim/optimizer.py\", line 76, in _use_grad\n",
      "    ret = func(self, *args, **kwargs)\n",
      "  File \"/opt/homebrew/lib/python3.8/site-packages/torch/optim/adam.py\", line 163, in step\n",
      "    adam(\n",
      "  File \"/opt/homebrew/lib/python3.8/site-packages/torch/optim/adam.py\", line 311, in adam\n",
      "    func(params,\n",
      "  File \"/opt/homebrew/lib/python3.8/site-packages/torch/optim/adam.py\", line 385, in _single_tensor_adam\n",
      "    exp_avg_sq.mul_(beta2).addcmul_(grad, grad.conj(), value=1 - beta2)\n",
      "KeyboardInterrupt\n",
      "[W 2024-02-28 16:07:49,310] Trial 36 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 68\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Optuna 최적화 실행\u001b[39;00m\n\u001b[1;32m     67\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 시도 횟수는 10으로 설정\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBest trial:\u001b[39m\u001b[38;5;124m'\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms value:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mvalue)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/optuna/study/study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \n\u001b[1;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[17], line 47\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     45\u001b[0m         loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y_batch)\n\u001b[1;32m     46\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 47\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# 검증 손실 계산\u001b[39;00m\n\u001b[1;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/optim/adam.py:163\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    152\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    155\u001b[0m         group,\n\u001b[1;32m    156\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    161\u001b[0m         state_steps)\n\u001b[0;32m--> 163\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/optim/adam.py:311\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    309\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 311\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m     \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m     \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m     \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m     \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m     \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m     \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m     \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m     \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m     \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m     \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m     \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m     \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.8/site-packages/torch/optim/adam.py:385\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    384\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 385\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    388\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # 학습 3: Optuna + CV 추가\n",
    "# import optuna\n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# def objective(trial):\n",
    "#     # K-Fold 교차 검증 설정\n",
    "#     # k = 5  # 분할 수\n",
    "#     # kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "#     # TimeSeriesSplit 설정\n",
    "#     tscv = TimeSeriesSplit(n_splits=5, gap=0) # gap: valid 전 train 데이터의 마지막 몇개 데이터 포인트를 제거하느냐\n",
    "\n",
    "#     # 하이퍼파라미터 탐색 공간 정의\n",
    "#     input_size = X.size(-1) # LSTM: X.shape[2](n_features), CNN-LSTM: X.size() -> CNN내에서 X.size(1)(seq_len)으로 input 실행\n",
    "#     hidden_size = trial.suggest_int('hidden_size', 5, 100, step=5)\n",
    "#     num_layers = trial.suggest_int('num_layers', 2, 5)\n",
    "#     lr = trial.suggest_categorical('lr', [0.1, 0.01, 0.001, 0.0001])\n",
    "#     num_epochs = 100  # 에폭 수는 고정값으로 설정\n",
    "\n",
    "#     # 교차 검증을 위한 전체 손실 초기화\n",
    "#     total_loss = 0.0\n",
    "\n",
    "#     for train_idx, val_idx in tscv.split(X):\n",
    "#         # 훈련 데이터와 검증 데이터로 분할\n",
    "#         X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "#         y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "#         # DataLoader 설정\n",
    "#         train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "#         val_dataset = TensorDataset(X_val_fold, y_val_fold)\n",
    "#         train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "#         val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "#         # 모델, 손실 함수, 옵티마이저 정의\n",
    "#         model = CNNLSTMModel(input_size, hidden_size, num_layers, num_classes=1).to(device)\n",
    "#         criterion = nn.BCEWithLogitsLoss() # 시그모이드 활성화 함수가 내장되어 있음. 모델의 마지막 레이어에서 시그모이드 함수 별도 적용할 필요X\n",
    "#         optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "#         # 모델 훈련\n",
    "#         model.train()\n",
    "#         for epoch in range(num_epochs):\n",
    "#             for x_batch, y_batch in train_loader:\n",
    "#                 x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 outputs = model(x_batch)\n",
    "#                 loss = criterion(outputs, y_batch)\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#         # 검증 손실 계산\n",
    "#         model.eval()\n",
    "#         with torch.no_grad():\n",
    "#             val_loss = 0.0\n",
    "#             for x_batch, y_batch in val_loader:\n",
    "#                 x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "#                 outputs = model(x_batch)\n",
    "#                 loss = criterion(outputs, y_batch)\n",
    "#                 val_loss += loss.item()\n",
    "#             val_loss /= len(val_loader)\n",
    "\n",
    "#         total_loss += val_loss\n",
    "\n",
    "#     # 평균 검증 손실을 반환\n",
    "#     avg_loss = total_loss / 5\n",
    "#     return avg_loss\n",
    "\n",
    "# # Optuna 최적화 실행\n",
    "# study = optuna.create_study(direction='minimize')\n",
    "# study.optimize(objective, n_trials=50)  # 시도 횟수는 10으로 설정\n",
    "\n",
    "# print('Best trial:', study.best_trial.params)\n",
    "# print(\"Best trial's value:\", study.best_trial.value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Optuna(with. Pruner) + CV 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-28 21:20:34,197] A new study created in memory with name: no-name-8852c764-fbe6-48e5-bb20-b0cdeaf9c5cd\n",
      "c:\\Users\\com\\anaconda3\\Lib\\site-packages\\optuna\\trial\\_trial.py:499: UserWarning: The reported value is ignored because this `step` 99 is already reported.\n",
      "  warnings.warn(\n",
      "[I 2024-02-28 21:22:12,960] Trial 0 finished with value: 0.6910429483652114 and parameters: {'hidden_size': 96, 'num_layers': 3, 'lr': 0.01}. Best is trial 0 with value: 0.6910429483652114.\n",
      "[I 2024-02-28 21:23:53,114] Trial 1 finished with value: 2.064844066500664 and parameters: {'hidden_size': 64, 'num_layers': 3, 'lr': 0.001}. Best is trial 0 with value: 0.6910429483652114.\n",
      "[I 2024-02-28 21:25:38,778] Trial 2 finished with value: 1.4596738010644912 and parameters: {'hidden_size': 96, 'num_layers': 5, 'lr': 0.001}. Best is trial 0 with value: 0.6910429483652114.\n"
     ]
    }
   ],
   "source": [
    "# 학습 4: Optuna(with. Pruner) + CV 추가\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "\n",
    "def objective(trial):\n",
    "    tscv = TimeSeriesSplit(n_splits=5, gap=0)\n",
    "\n",
    "    input_size = X.size(-1)\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 256, step=32)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 5)\n",
    "    lr = trial.suggest_categorical('lr', [0.01, 0.001, 0.0001])\n",
    "    num_epochs = 100\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for train_idx, val_idx in tscv.split(X):\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "        train_dataset = TensorDataset(X_train_fold, y_train_fold)\n",
    "        val_dataset = TensorDataset(X_val_fold, y_val_fold)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "        model = CNNLSTMModel(input_size, hidden_size, num_layers, num_classes=1).to(device)\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(x_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(x_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "            val_loss /= len(val_loader)\n",
    "\n",
    "        total_loss += val_loss\n",
    "\n",
    "        # Pruner를 위한 조기 중단 로직\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    avg_loss = total_loss / 5\n",
    "    return avg_loss\n",
    "\n",
    "# MedianPruner 초기화 및 Optuna 최적화 실행\n",
    "pruner = MedianPruner()\n",
    "study = optuna.create_study(direction='minimize', pruner=pruner)\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "print('Best trial:', study.best_trial.params)\n",
    "print(\"Best trial's value:\", study.best_trial.value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (1180583119.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    best_model = CNNLSTMModel(input_size=input_size, best_params['hidden_size'], best_params['num_layers'], num_classes=1).to(device)\u001b[0m\n\u001b[0m                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# 최적의 하이퍼파라미터로 모델 평가\n",
    "best_params = study.best_trial.params\n",
    "\n",
    "input_size = X_test_seq.size(-1)\n",
    "\n",
    "# 모델을 최적의 하이퍼파라미터로 초기화\n",
    "best_model = CNNLSTMModel(input_size, best_params['hidden_size'], best_params['num_layers'], num_classes=1).to(device)\n",
    "\n",
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(best_model.parameters(), lr=best_params['lr'])\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "best_model.eval()\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# 검증 데이터로 평가\n",
    "with torch.no_grad(): # 기울기 계산X -> 메모리 사용량, 속도 줄어듬\n",
    "    val_loss = 0.0\n",
    "    for x_batch, labels in test_loader:\n",
    "        x_batch, labels = x_batch.to(device), labels.to(device)\n",
    "        # 로그 오즈를 확률로 변환\n",
    "        probs = torch.sigmoid(outputs).squeeze()\n",
    "        # 확률을 기준으로 0.5 이상이면 1, 미만이면 0으로 예측\n",
    "        preds = torch.round(probs).cpu().numpy()\n",
    "        y_true.extend(labels.squeeze().cpu().numpy())\n",
    "        y_pred.extend(preds)\n",
    "\n",
    "# 성능 지표 계산\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# 결과 출력\n",
    "print(f'Accuracy: {accuracy.round(4)}')\n",
    "print(f'Precision: {precision.round(4)}')\n",
    "print(f'Recall: {recall.round(4)}')\n",
    "print(f'F1 Score: {f1.round(4)}')\n",
    "\n",
    "# 혼동 행렬 출력\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "sns.heatmap(conf_mat, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
