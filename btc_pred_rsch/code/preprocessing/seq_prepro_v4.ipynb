{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### num_rows 기준 정하고 그에 해당하는 시퀀스 모두 제거하는 코드\n",
    "\n",
    "- 코드설명\n",
    "1. del_idx 컬럼의 값을 가진 csv file 불러오기 (bitcoin_prepro_v2.ipynb에서 작업)\n",
    "2. num_rows 기준에 해당하는 값과 그 값이 포함된 시퀀스 제외한 최종 시퀀스 생성\n",
    "3. 시퀀스 정규화(MinMaxScale) 및 tensor 형태로 변환\n",
    "4. time gap이 존재하는 부분은 sequence에서 제거\n",
    "5. 모든 sequence 기준(20, 40, ... 320)에 따른 pickle 파일 생성 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이 PC는 윈도우 운영 체제입니다: cuda:0 is available\n"
     ]
    }
   ],
   "source": [
    "# 필요 라이브러리 import\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dataset 관련\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sequence as sq # 사용자 정의 함수 불러오기\n",
    "\n",
    "# 성능 평가 관련\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization 관련\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 하이퍼파라미터 튜닝\n",
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# 운영체제 관련\n",
    "import platform\n",
    "\n",
    "'''\n",
    "딥러닝 학습을 진행할 때, 가중치를 임의의 값으로 초기화하여 학습을 수행하는 데, \n",
    "실험을 동일하게 진행하기 위해서는 난수를 동일하게 생성해야 한다.\n",
    "Pytorch에서 random seed를 고정하기 위해 manual_seed를 사용한다.\n",
    "'''\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)  # 멀티 GPU 사용 시\n",
    "# GPU에서 실행할 때, CUDNN 자동 튜너의 비결정적 행동을 방지하기 위해 이를 설정\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# 운영체제별 device 설정\n",
    "os_name = platform.system()\n",
    "if os_name == 'Windows':\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"이 PC는 윈도우 운영 체제입니다: {device} is available\")\n",
    "elif os_name == 'Darwin':\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available else \"cpu\")\n",
    "    print(f\"이 PC는 맥(OS X) 운영 체제입니다: {device} is available\")\n",
    "else:\n",
    "    print(f\"이 PC는 다른 운영 체제입니다: {os_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sequence' has no attribute 'createSeqForBacktestwithTimeGapDelete'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m seq_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m \u001b[38;5;66;03m# 20, 40, 80, 160, 320\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m#X, y = sq.create_sequence(df, seq_len=seq_len) # 사용자 정의 함수\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m X, y, y_for_backtest \u001b[38;5;241m=\u001b[39m sq\u001b[38;5;241m.\u001b[39mcreateSeqForBacktestwithTimeGapDelete(df, seq_len\u001b[38;5;241m=\u001b[39mseq_len)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# timegap 포함되는 sequence도 삭제. 수행 결과 데이터의 개수가 6만개 정도만 남는것으로 거의 반토막남.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#X, y, y_for_backtest = sq.createSeqForBacktestwithTimeGapDelete(df, seq_len=seq_len)\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Tensor화\u001b[39;00m\n\u001b[0;32m     23\u001b[0m X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(X)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sequence' has no attribute 'createSeqForBacktestwithTimeGapDelete'"
     ]
    }
   ],
   "source": [
    "# 데이터 불러오기\n",
    "file_path = '../../data/' # 경로 설정\n",
    "df = pd.read_csv(file_path + 'bitcoin_data_num_rows_gt_5.csv')\n",
    "#df = df.iloc[:10000]\n",
    "#df['returns_next10m'] = df['returns_next10m'].apply(lambda x: 0 if x <= 0 else 1) # 종속변수 이진분류화\n",
    "\n",
    "# transaction fee를 고려한 종속변수\n",
    "# transaction_fee = 0.001\n",
    "# df['returns_next10m'] = df['returns_next10m'] - transaction_fee # subtract transaction fee\n",
    "\n",
    "df['returns_next10m_binary'] = df['returns_next10m'].apply(lambda x: 0 if x <= 0 else 1) # 종속변수 이진분류화\n",
    "df = df.sort_values(by='window_start', ascending=True) # 시간순 정렬\n",
    "\n",
    "# sequence length를 기준으로 sequence 데이터 생성\n",
    "seq_len = 20 # 20, 40, 80, 160, 320\n",
    "#X, y = sq.create_sequence(df, seq_len=seq_len) # 사용자 정의 함수\n",
    "X, y, y_for_backtest = sq.createSeqForBacktestwithTimeGapDelete(df, seq_len=seq_len)\n",
    "\n",
    "# timegap 포함되는 sequence도 삭제. 수행 결과 데이터의 개수가 6만개 정도만 남는것으로 거의 반토막남.\n",
    "#X, y, y_for_backtest = sq.createSeqForBacktestwithTimeGapDelete(df, seq_len=seq_len)\n",
    "\n",
    "# Tensor화\n",
    "X = torch.FloatTensor(X).to(device)\n",
    "y = torch.FloatTensor(y).to(device)\n",
    "print('Full Data Size:', X.size(), y.size())\n",
    "\n",
    "# split (60% / 20% / 20%)\n",
    "train_split = int((X.size(0)) * 0.6)\n",
    "valid_split = int((X.size(0)) * 0.8)\n",
    "\n",
    "X_train_seq = X[:train_split]\n",
    "X_val_seq = X[train_split:valid_split]\n",
    "X_test_seq = X[valid_split:]\n",
    "y_train_seq = y[:train_split]\n",
    "y_val_seq = y[train_split:valid_split]\n",
    "y_test_seq = y[valid_split:]\n",
    "y_test_bt = y_for_backtest[valid_split:] # for backtest\n",
    "\n",
    "print('Train Size:', X_train_seq.size(), y_train_seq.size())\n",
    "print('Valid Size:', X_val_seq.size(), y_val_seq.size())\n",
    "print('Test Size:', X_test_seq.size(), y_test_seq.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for verification\n",
    "# verification_row = df[df['window_start'] == '2022-12-16 21:05:30'][['window_start',\n",
    "#                                                                     'lowest_return',\n",
    "#                                                                     'highest_return',\n",
    "#                                                                     'highest_possible_return',\n",
    "#                                                                     'returns_next10m']]\n",
    "# verification_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### pickle 파일 생성 및 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 sequence data export finished\n"
     ]
    }
   ],
   "source": [
    "# Pickle 모듈 사용하여 리스트 파일로 내보내기\n",
    "import os\n",
    "import pickle\n",
    "file_path = '../../data/'\n",
    "\n",
    "# 폴더가 없으면 생성\n",
    "if not os.path.exists(file_path):\n",
    "    os.makedirs(file_path)\n",
    "\n",
    "# 리스트 파일로 저장\n",
    "with open(f'{file_path}x_train_seq_{seq_len}.pkl', 'wb') as file: # x_train\n",
    "    pickle.dump(X_train_seq, file)\n",
    "\n",
    "with open(f'{file_path}x_val_seq_{seq_len}.pkl', 'wb') as file: # x_valid\n",
    "    pickle.dump(X_val_seq, file)\n",
    "\n",
    "with open(f'{file_path}y_train_seq_{seq_len}.pkl', 'wb') as file: # y_train\n",
    "    pickle.dump(y_train_seq, file)\n",
    "\n",
    "with open(f'{file_path}x_test_seq_{seq_len}.pkl', 'wb') as file: # x_test\n",
    "    pickle.dump(X_test_seq, file)\n",
    "\n",
    "with open(f'{file_path}y_val_seq_{seq_len}.pkl', 'wb') as file: # y_valid\n",
    "    pickle.dump(y_val_seq, file)\n",
    "\n",
    "with open(f'{file_path}y_test_seq_{seq_len}.pkl', 'wb') as file: # y_test\n",
    "    pickle.dump(y_test_seq, file)\n",
    "\n",
    "with open(f'{file_path}y_test_bt_{seq_len}.pkl', 'wb') as file: # y_test_bt\n",
    "    pickle.dump(y_test_bt, file)\n",
    "\n",
    "print(f'{seq_len} sequence data export finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불러오기\n",
    "with open(f'{file_path}x_train_seq_{seq_len}.pkl', 'rb') as file:\n",
    "     loaded_lst= pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mac에서 돌리면 kernel 뻑남..\n",
    "# tmp_seq = []\n",
    "\n",
    "# # 가정: create_sequence 함수는 이미 정의되어 있으며, df 데이터프레임과 sequence_length 값을 입력으로 받음\n",
    "# sequence_lengths = [20, 40, 80, 160, 320]\n",
    "\n",
    "# # 리스트 컴프리헨션을 사용하여 각 sequence_length에 대한 결과의 길이 계산\n",
    "# tmp_seq = [len(create_sequence(df, length)) for length in sequence_lengths]\n",
    "\n",
    "# # 결과를 pandas DataFrame으로 변환\n",
    "# result_df = pd.DataFrame({\n",
    "#     'sequence_length': sequence_lengths,\n",
    "#     'length': tmp_seq\n",
    "# })\n",
    "\n",
    "# result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data export\n",
    "#result_df.to_csv(file_path+'seq_data_counts.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([111077, 20, 77]) torch.Size([111077, 1])\n"
     ]
    }
   ],
   "source": [
    "X, y = sequence.create_sequence(df, 20)\n",
    "print(X.size(), y.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
