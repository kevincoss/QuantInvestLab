{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f66688f-0347-493c-88ec-997e71aa7911",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# STEP1: 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80de464b-dc5b-4819-bb40-2f9370a81eda",
   "metadata": {},
   "source": [
    "## 1-0: 전체 파일을 코인 별로 나누어 저장\n",
    "\n",
    "<!-- 전체파일을 sort_values 해보자. (시간순으로 정렬) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "923065c2-92c9-45d1-baad-4bdef12bcaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "\n",
    "# Change \"current directory\"\n",
    "new_dir = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "# new_dir = 'C:\\\\Users\\\\user\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "os.chdir(new_dir)\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import dask.dataframe as dd # This is a main package to process a large csv file.\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pickle\n",
    "import pandas as pd \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d449e17c-ac93-4593-872f-5de0cf2e34de",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19180\\2129804284.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# in_csv =\"D:\\\\ticker_data_{}.csv\".format(data_id)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0min_csv\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\Data\\\\ticker_data_{}.csv\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m dtype={'ask_bid': 'object',\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_id' is not defined"
     ]
    }
   ],
   "source": [
    "# data_id = 202302230905 \n",
    "# data_id = 202311281936 \n",
    "# data_id = 202303271051\n",
    "# data_id = 202312021632 \n",
    "# data_id = 202312081015\n",
    "# data_id = '202312122122'\n",
    "# 202302230905\n",
    "#  202303271051 # 실질적으로는 12/16부터 2/26까지 데이터 존재.\n",
    "\n",
    "# in_csv =\"D:\\\\ticker_data_{}.csv\".format(data_id)\n",
    "in_csv =\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\Data\\\\ticker_data_{}.csv\".format(data_id)\n",
    "\n",
    "dtype={'ask_bid': 'object',\n",
    "       'change': 'object',\n",
    "       'highest_52_week_date': 'object',\n",
    "       'lowest_52_week_date': 'object',\n",
    "       'market_state': 'object',\n",
    "       'market_warning': 'object',\n",
    "       'orderbook_ap_0': 'float32',\n",
    "       'orderbook_ap_1': 'float32',\n",
    "       'orderbook_ap_2': 'float32',\n",
    "       'orderbook_ap_3': 'float32',\n",
    "       'orderbook_bp_0': 'float32',\n",
    "       'orderbook_bp_1': 'float32',\n",
    "       'orderbook_bp_10': 'float32',\n",
    "       'orderbook_bp_11': 'float32',\n",
    "       'orderbook_bp_12': 'float32',\n",
    "       'orderbook_bp_13': 'float32',\n",
    "       'orderbook_bp_14': 'float32',\n",
    "       'orderbook_bp_2': 'float32',\n",
    "       'orderbook_bp_3': 'float32',\n",
    "       'orderbook_bp_4': 'float32',\n",
    "       'orderbook_bp_5': 'float32',\n",
    "       'orderbook_bp_6': 'float32',\n",
    "       'orderbook_bp_7': 'float32',\n",
    "       'orderbook_bp_8': 'float32',\n",
    "       'orderbook_bp_9': 'float32',\n",
    "       'stream_type': 'object',\n",
    "       'trade_time': 'object'}\n",
    "\n",
    "# Read the CSV file using a Dask DataFrame\n",
    "df = dd.read_csv(in_csv, dtype=dtype) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4955a86c-5c6a-421a-94e5-f1f610923808",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23856\\112092153.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# data_num = 3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mcoin\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcoin_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"df_temp\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"df\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mglobals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"df\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mf'KRW-{coin}'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;31m# with open(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\t_data_{}_{}.csv\".format(coin, data_id), 'wb') as file:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#     pickle.dump(df_temp, file)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'df'"
     ]
    }
   ],
   "source": [
    "## 수집된 파일에는 n개의 코인이 함께 저장되어 있음. 따라서, Separate files by COIN\n",
    "\n",
    "coin_list = ['BTC'] #, 'ETH', 'DOGE', 'XRP', 'LINK', 'AVAX', 'SOL']\n",
    "\n",
    "for coin in coin_list:\n",
    "    globals()[\"df_temp\"] = globals()[\"df\"][globals()[\"df\"].code == f'KRW-{coin}']\n",
    "    # with open(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\t_data_{}_{}.csv\".format(coin, data_id), 'wb') as file:\n",
    "    #     pickle.dump(df_temp, file)\n",
    "    df_temp.to_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\t_data_{}_{}.csv\".format(coin, data_id))\n",
    "    print('Complete: {}'.format(coin))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7802b4-c1cb-488f-8d04-cd4bca445c94",
   "metadata": {},
   "source": [
    "## 1-1: 코인별로 나누어진 파일을, 시간순으로 sorting 후 다시 저장\n",
    "### duplicates 제거 및 30초/60초 ticker 값을 제거\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfbc2f7-6e5c-43ff-83b7-05b9fc68b1a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "#USE ONLY ONE OF THESE:\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "\n",
    "# Change \"current directory\"\n",
    "new_dir = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "os.chdir(new_dir)\n",
    "\n",
    "# Import packages\n",
    "import numpy as np\n",
    "import csv\n",
    "import dask.dataframe as dd # This is a main package to process a large csv file.\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "# import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f4e889-9748-4b1f-9aa7-aff6f73f52e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete: Sorting\n"
     ]
    }
   ],
   "source": [
    "dtype={'ask_bid': 'object',\n",
    "       'change': 'object',\n",
    "       'highest_52_week_date': 'object',\n",
    "       'lowest_52_week_date': 'object',\n",
    "       'market_state': 'object',\n",
    "       'market_warning': 'object',\n",
    "       'orderbook_ap_0': 'float32',\n",
    "       'orderbook_ap_1': 'float32',\n",
    "       'orderbook_ap_2': 'float32',\n",
    "       'orderbook_ap_3': 'float32',\n",
    "       'orderbook_bp_0': 'float32',\n",
    "       'orderbook_bp_1': 'float32',\n",
    "       'orderbook_bp_10': 'float32',\n",
    "       'orderbook_bp_11': 'float32',\n",
    "       'orderbook_bp_12': 'float32',\n",
    "       'orderbook_bp_13': 'float32',\n",
    "       'orderbook_bp_14': 'float32',\n",
    "       'orderbook_bp_2': 'float32',\n",
    "       'orderbook_bp_3': 'float32',\n",
    "       'orderbook_bp_4': 'float32',\n",
    "       'orderbook_bp_5': 'float32',\n",
    "       'orderbook_bp_6': 'float32',\n",
    "       'orderbook_bp_7': 'float32',\n",
    "       'orderbook_bp_8': 'float32',\n",
    "       'orderbook_bp_9': 'float32',\n",
    "       'stream_type': 'object',\n",
    "       'trade_time': 'object'}\n",
    "\n",
    "# data_id = 202303271051 # 해당 틱데이터 커버 기간: 22년 12월 16일 오후 9시 ~ 23년 2월 26일 오전 4시\n",
    "# coin_list = [ 'BTC',  'ETH',  'DOGE', 'XRP'] \n",
    "\n",
    "for coin in coin_list:\n",
    "\n",
    "    # Read dask \"part\" files\n",
    "    part_files_path =  \"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\t_data_{}_{}.csv\\\\\".format(coin, data_id)\n",
    "    df = dd.read_csv( part_files_path + '*.part', dtype=dtype)    \n",
    "    # df = dd.read_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\t_data_{}_{}.csv\".format(coin, data_id))\n",
    "    # with open('.\\\\output\\\\t_data_{}_{}.pkl'.format(coin, data_id), 'rb') as file:\n",
    "    #     loaded_df = pickle.load(file)\n",
    "\n",
    "    # Sort dataframe\n",
    "    df = df.sort_values('sys_datetime', ascending = True, na_position = 'last')\n",
    "    print('Complete: Sorting')\n",
    "    \n",
    "    # Write the sorted DataFrame to a new dask-type csv file\n",
    "    df.to_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\s_t_data_{}_{}.csv\".format(coin, data_id), index=False)\n",
    "\n",
    "    # with open(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\s_t_data_{}_{}.csv\".format(coin, data_id), 'wb') as file:\n",
    "    #     pickle.dump(loaded_df, file)\n",
    "\n",
    "    # # Write the sorted DataFrame to a new parquet file\n",
    "    # df.to_parquet('s_t_data_{}_{}.parquet'.format(coin,data_id), engine='pyarrow')\n",
    "\n",
    "## Note: 용량 부족 메시지가 떠서, 한 코인씩 돌림. (for 문을 돌리지 못함.)    \n",
    "## Note: parquet으로는 저장되지 않음 (메모리 문제라는 메시지가 뜸.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44950a7",
   "metadata": {},
   "source": [
    "### duplicates 제거 및 30초/60초 ticker 값을 제거 (단위 시간마다 신규 거래가 없더라도 ticker 값이 들어온다)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66158477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.shape 1 : (10606368, 99)\n",
      "df.shape 2 : (10606368, 99)\n",
      "df.shape 3 : (9506748, 99)\n",
      "Complete: Drop duplicates\n",
      "Complete: File Saving\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "new_dir = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "os.chdir(new_dir)\n",
    "import numpy as np\n",
    "import csv\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import gc\n",
    "\n",
    "\n",
    "dtype={'ask_bid': 'object',\n",
    "       'change': 'object',\n",
    "       'highest_52_week_date': 'object',\n",
    "       'lowest_52_week_date': 'object',\n",
    "       'market_state': 'object',\n",
    "       'market_warning': 'object',\n",
    "       'orderbook_ap_0': 'float32',\n",
    "       'orderbook_ap_1': 'float32',\n",
    "       'orderbook_ap_2': 'float32',\n",
    "       'orderbook_ap_3': 'float32',\n",
    "       'orderbook_bp_0': 'float32',\n",
    "       'orderbook_bp_1': 'float32',\n",
    "       'orderbook_bp_10': 'float32',\n",
    "       'orderbook_bp_11': 'float32',\n",
    "       'orderbook_bp_12': 'float32',\n",
    "       'orderbook_bp_13': 'float32',\n",
    "       'orderbook_bp_14': 'float32',\n",
    "       'orderbook_bp_2': 'float32',\n",
    "       'orderbook_bp_3': 'float32',\n",
    "       'orderbook_bp_4': 'float32',\n",
    "       'orderbook_bp_5': 'float32',\n",
    "       'orderbook_bp_6': 'float32',\n",
    "       'orderbook_bp_7': 'float32',\n",
    "       'orderbook_bp_8': 'float32',\n",
    "       'orderbook_bp_9': 'float32',\n",
    "       'stream_type': 'object',\n",
    "       'trade_time': 'object'}\n",
    "\n",
    "data_id = '202312122122'\n",
    "# data_id = '202303271051' # 해당 틱데이터 커버 기간: 22년 12월 16일 오후 9시 ~ 23년 2월 26일 오전 4시\n",
    "# coin_list = ['BTC', 'ETH',  'DOGE', 'XRP'] \n",
    "coin_list = ['BTC'] \n",
    "for coin in coin_list:\n",
    "\n",
    "    # with open('.\\\\output\\\\s_t_data_{}_{}.csv'.format(coin, data_id), 'rb') as file:\n",
    "    #     loaded_df = pickle.load(file)\n",
    "\n",
    "    ### Read dask \"part\" failes\n",
    "    part_files_path =  \"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\s_t_data_{}_{}.csv\\\\\".format(coin, data_id)\n",
    "    df = dd.read_csv( part_files_path + '*.part', dtype=dtype)    \n",
    "    df = df.compute()\n",
    "\n",
    "    gc.collect()\n",
    "\n",
    "    ### Drop duplicates\n",
    "    print(\"df.shape 1 :\", df.shape)\n",
    "    df.drop_duplicates(subset=[ 'type_websocket', 'timestamp','sys_datetime'], keep='first', inplace=True, ignore_index=True)\n",
    "    print(\"df.shape 2 :\", df.shape)\n",
    "    df.drop_duplicates(subset=[ 'type_websocket',  'opening_price',  'high_price',   'low_price',  'trade_price',  'prev_closing_price', 'change',  'change_price',  'signed_change_price',  'change_rate',  'signed_change_rate',  'trade_volume',  'acc_trade_volume',   'acc_trade_price',    'trade_date', 'ask_bid',  'acc_ask_volume',  'acc_bid_volume',  'highest_52_week_price', 'highest_52_week_date',  'lowest_52_week_price', 'lowest_52_week_date', 'market_state',  'is_trading_suspended'   , 'total_ask_size',  'total_bid_size',  'orderbook_ap_0',  'orderbook_as_0',  'orderbook_bp_0',  'orderbook_bs_0',  'orderbook_ap_1',  'orderbook_as_1',  'orderbook_bp_1',  'orderbook_bs_1',  'orderbook_ap_2',  'orderbook_as_2',  'orderbook_bp_2',  'orderbook_bs_2',  'orderbook_ap_3',  'orderbook_as_3',  'orderbook_bp_3',  'orderbook_bs_3',  'orderbook_ap_4',  'orderbook_as_4',  'orderbook_bp_4',  'orderbook_bs_4',  'orderbook_ap_5',  'orderbook_as_5',  'orderbook_bp_5',  'orderbook_bs_5',  'orderbook_ap_6',  'orderbook_as_6',  'orderbook_bp_6',  'orderbook_bs_6',  'orderbook_ap_7',  'orderbook_as_7',  'orderbook_bp_7',  'orderbook_bs_7',  'orderbook_ap_8',  'orderbook_as_8',  'orderbook_bp_8',  'orderbook_bs_8',  'orderbook_ap_9',  'orderbook_as_9',  'orderbook_bp_9',  'orderbook_bs_9',  'orderbook_ap_10',  'orderbook_as_10',  'orderbook_bp_10',  'orderbook_bs_10',  'orderbook_ap_11',  'orderbook_as_11',  'orderbook_bp_11',  'orderbook_bs_11',  'orderbook_ap_12',  'orderbook_as_12',  'orderbook_bp_12',  'orderbook_bs_12',  'orderbook_ap_13',  'orderbook_as_13',  'orderbook_bp_13',  'orderbook_bs_13',  'orderbook_ap_14',  'orderbook_as_14',  'orderbook_bp_14',  'orderbook_bs_14'    ], keep='first', inplace=True, ignore_index=True)\n",
    "    print(\"df.shape 3 :\", df.shape)\n",
    "\n",
    "    print('Complete: Drop duplicates')\n",
    "\n",
    "    ### Write the sorted DataFrame to a new dask-type csv file\n",
    "    df.to_csv(\"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\s_t_d_data_{}_{}.csv\".format(coin, data_id), index=False)\n",
    "    print('Complete: File Saving')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387d5e5e",
   "metadata": {},
   "source": [
    "## 1-2(b): 각 코인의 틱데이터를 10분씩 묶는 전처리 작업 (rolling window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85063092",
   "metadata": {},
   "source": [
    "### Setting & Running => Output: {}_sum_both_10m.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57fb2e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version Number\n",
    "\n",
    "data_id = 202303271051\n",
    "# data_id = 202312122122\n",
    "coin = 'XRP'\n",
    "\n",
    "### Libraries and key functions\n",
    "import os\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "\n",
    "working_directory = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "os.chdir(working_directory)\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import gc\n",
    "import dask.dataframe as dd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "from contextlib import contextmanager  # 컨텍스트 관리자를 사용하기 위한 contextlib 모듈을 가져옵니다.\n",
    "import time  # 시간 관련 기능을 사용하기 위한 time 모듈을 가져옵니다.\n",
    "\n",
    "\n",
    "def log_return(series: np.ndarray):\n",
    "    return np.log(series).diff()\n",
    "\n",
    "def realized_volatility(series):\n",
    "    return np.sqrt(np.sum(series ** 2))\n",
    "\n",
    "def flatten_name(prefix, src_names):\n",
    "    ret = []\n",
    "    for c in src_names:\n",
    "        if c[0] in ['time_id', 'stock_id']:\n",
    "            ret.append(c[0])\n",
    "        else:\n",
    "            ret.append('.'.join([prefix] + list(c)))\n",
    "    return ret\n",
    "\n",
    "@contextmanager\n",
    "def timer(name: str):\n",
    "    s = time.time()\n",
    "    yield\n",
    "    elapsed = time.time() - s\n",
    "    print(f'[{name}] {elapsed: .3f}초')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32fdd003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#USE ONLY ONE OF THESE:\n",
    "# os.environ[\"MODIN_ENGINE\"] = \"ray\"  # Modin will use Ray\n",
    "os.environ[\"MODIN_ENGINE\"] = \"dask\"  # Modin will use Dask\n",
    "# Change \"current directory\"\n",
    "working_directory = 'D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\'\n",
    "os.chdir(working_directory)\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import dask.dataframe as dd # This is a main package to process a large csv file.\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd \n",
    "import gc\n",
    "\n",
    "dtype={'ask_bid': 'object',\n",
    "       'change': 'object',\n",
    "       'highest_52_week_date': 'object',\n",
    "       'lowest_52_week_date': 'object',\n",
    "       'market_state': 'object',\n",
    "       'market_warning': 'object',\n",
    "       'orderbook_ap_0': 'float32',\n",
    "       'orderbook_ap_1': 'float32',\n",
    "       'orderbook_ap_2': 'float32',\n",
    "       'orderbook_ap_3': 'float32',\n",
    "       'orderbook_bp_0': 'float32',\n",
    "       'orderbook_bp_1': 'float32',\n",
    "       'orderbook_bp_10': 'float32',\n",
    "       'orderbook_bp_11': 'float32',\n",
    "       'orderbook_bp_12': 'float32',\n",
    "       'orderbook_bp_13': 'float32',\n",
    "       'orderbook_bp_14': 'float32',\n",
    "       'orderbook_bp_2': 'float32',\n",
    "       'orderbook_bp_3': 'float32',\n",
    "       'orderbook_bp_4': 'float32',\n",
    "       'orderbook_bp_5': 'float32',\n",
    "       'orderbook_bp_6': 'float32',\n",
    "       'orderbook_bp_7': 'float32',\n",
    "       'orderbook_bp_8': 'float32',\n",
    "       'orderbook_bp_9': 'float32',\n",
    "       'stream_type': 'object',\n",
    "       'trade_time': 'object'}\n",
    "\n",
    "# with open('.\\\\output\\\\s_t_d_data_{}_{}.pkl'.format(coin, data_id), 'rb') as file:\n",
    "#        df = pickle.load(file)\n",
    "# s_t_d_data_BTC_202312122122\n",
    "\n",
    "## 저장이 Dask dataframe으로 되어 있으면 아래 코드를 실행\n",
    "part_files_path =  \"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\s_t_d_data_{}_{}.csv\\\\\".format(coin, data_id)\n",
    "df = dd.read_csv( part_files_path + '*.part', dtype=dtype) \n",
    "\n",
    "# # ## 저장이 pandas dataframe으로 되어 있으면 아래 코드를 실행\n",
    "# part_files_path =  \"D:\\\\OneDrive - 한동대학교\\\\PROJECT\\\\트머프로젝트\\\\output\\\\s_t_d_data_{}_{}.csv\".format(coin, data_id)\n",
    "# df = pd.read_csv( part_files_path , dtype=dtype) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering (TICKER 부문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbeb16fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_ticker(df_ticker):\n",
    "\n",
    "    df_ticker_pd = df_ticker.copy()\n",
    "    df_ticker_pd.reset_index(inplace=True, drop=True)\n",
    "    df_ticker_pd['datetime'] = pd.to_datetime(df_ticker_pd['datetime'])\n",
    "    # df_ticker_pd['log_return'] = df_ticker_pd['trade_price'].pct_change().apply(lambda x: np.log(1 + x))\n",
    "\n",
    "    # # signed volume (to calculate volume power)\n",
    "    # df_ticker_pd['ask_bid'] = df_ticker_pd['ask_bid'].map({'ASK': -1, 'BID': 1})\n",
    "    # df_ticker_pd['signed_volume'] = df_ticker_pd['trade_volume'] * df_ticker_pd['ask_bid'] \n",
    "\n",
    "    # Set 'timestamp' as the index for resampling\n",
    "    df_ticker_pd.set_index('datetime', inplace=True)\n",
    "    # Resample data into 10-minute bins and take the last value in each bin\n",
    "    df_ticker_pd_1m = df_ticker_pd['trade_price'].resample('1T').last()\n",
    "    # Shift the 1-minute prices one step forward to align with the original seconds data\n",
    "    df_ticker_pd_1m = df_ticker_pd_1m.shift(1)\n",
    "    # Now, reindex the original DataFrame with 10-minute frequency and forward fill the 'ending_price' values\n",
    "    df_ticker_pd['ending_price_b1m'] = df_ticker_pd_1m.reindex(df_ticker_pd.index, method='ffill') # 이를 이용해서 10분 동안 얼마나 하락하는가 확인.\n",
    "    # Reset the index to get the DataFrame with the original 'datetime', 'trade_price', and 'ending_price' columns\n",
    "    df_ticker_pd.reset_index(inplace=True)\n",
    "    # datetime을 1분 단위로 정리하는 변수 (dt_1m)을 만듬 (이는 데이터를 '정리'하는 데에 활용될 단위임)\n",
    "    df_ticker_pd['dt_1m'] = df_ticker_pd['datetime'].dt.floor('1min').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "    # Set 'dt_1m' as the index for easier resampling\n",
    "    df_ticker_pd['dt_1m'] = pd.to_datetime(df_ticker_pd['dt_1m'])\n",
    "    df_ticker_pd.set_index('dt_1m', inplace=True, drop = False)\n",
    "    # Define the size of the moving window (10 minutes in this case)\n",
    "    window_size = pd.Timedelta(minutes=10)\n",
    "\n",
    "    # Create an empty list to store features for each rolling window\n",
    "    # realized_volatility = []\n",
    "    realized_volatility_mean0 = []\n",
    "    realized_up_volatility_mean0 = []\n",
    "    realized_down_volatility_mean0 = []\n",
    "    num_trades = []\n",
    "    lowest_return = []\n",
    "    trade_vol = []\n",
    "    volume_power = []\n",
    "    highest_return = []\n",
    "    high_low_gap = []\n",
    "    end_price = []\n",
    "    # prices_30s_for_NN = []\n",
    "    # num_of_trades = []\n",
    "    returns = []\n",
    "    trade_vol_top1p_ratio = []\n",
    "    volume_power_top1p = []\n",
    "\n",
    "    temp_index = df_ticker_pd.index.unique().tolist()\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    # Define the time range\n",
    "    start_time = pd.to_datetime('00:00:00').time()\n",
    "    end_time = pd.to_datetime('06:00:00').time()\n",
    "\n",
    "    window_start2 = []\n",
    "    window_end2 = []\n",
    "\n",
    "    # Iterate through each time window using the rolling method\n",
    "    for window_start in temp_index[:-10]:\n",
    "        window_end = window_start + window_size\n",
    "\n",
    "        if  (start_time <= window_start.time() <= end_time) or  (start_time <= window_end.time() <= end_time):\n",
    "            continue \n",
    "\n",
    "        \n",
    "\n",
    "        # print(\"window_start:\", window_start, \" & window_end:\", window_end)\n",
    "\n",
    "        # Filter the data for each rolling window\n",
    "        rolling_window_data = df_ticker_pd.loc[(df_ticker_pd.index >= window_start) & (df_ticker_pd.index < window_end)]\n",
    "        # previous_price = rolling_window_data['ending_price_b1m'].iloc[0] # 10분 bin이 시작하기 직전 마지막 trade price 값을 의미.\n",
    "\n",
    "        try: \n",
    "            previous_price = rolling_window_data['trade_price'].iloc[0]\n",
    "        except Exception as e:\n",
    "            print(\"No previous price.\")\n",
    "            continue\n",
    "\n",
    "        window_start2.append(window_start)\n",
    "        window_end2.append(window_end)\n",
    "        \n",
    "        # rolling_window_data['log_return_from_pp'] = np.log(rolling_window_data['trade_price'] / previous_price)\n",
    "        rolling_window_data.loc[:, 'log_return_from_pp'] = np.log(rolling_window_data['trade_price'] / previous_price)\n",
    "\n",
    "        # VARIABLE: realized_volatility \n",
    "        # Calculate the returns (percentage change) for each tick in the rolling window\n",
    "        returns2 = rolling_window_data['log_return_from_pp']\n",
    "        # Calculate the volatility as the standard deviation of returns\n",
    "        # volatility = returns.std()\n",
    "        # Append the average volatility to the list\n",
    "        # realized_volatility.append(volatility)\n",
    "\n",
    "        # returns_from_pp = rolling_window_data['log_return_from_pp']\n",
    "        squared_returns = np.square(returns2)\n",
    "        average_squared_returns = np.mean(squared_returns)\n",
    "        realized_volatility_mean0.append(np.sqrt(average_squared_returns))\n",
    "\n",
    "        squared_returns_up = np.square(returns2[returns2 >= 0])\n",
    "        squared_returns_down = np.square(returns2[returns2 < 0])\n",
    "        average_squared_returns_up = np.mean(squared_returns_up)\n",
    "        average_squared_returns_down = np.mean(squared_returns_down)\n",
    "        realized_up_volatility_mean0.append(np.sqrt(average_squared_returns_up))\n",
    "        realized_down_volatility_mean0.append(np.sqrt(average_squared_returns_down))\n",
    "\n",
    "        # VARIABLE: number of trades \n",
    "        # Calculate the volatility as the standard deviation of returns\n",
    "        n_trades = rolling_window_data.shape[0]\n",
    "        # Append n_trades to the list\n",
    "        num_trades.append(n_trades)\n",
    "\n",
    "        # VARIABLE: lowest return\n",
    "        # Calculate the lowest return\n",
    "        l_return = rolling_window_data['log_return_from_pp'].min()\n",
    "        # Append n_trades to the list\n",
    "        lowest_return.append(l_return)\n",
    "\n",
    "        # VARIABLE: highest return\n",
    "        h_return = rolling_window_data['log_return_from_pp'].max()\n",
    "        highest_return.append(h_return) \n",
    "\n",
    "        # VARIABLE: high_low_gap\n",
    "        hl_gap = h_return - l_return\n",
    "        high_low_gap.append(hl_gap)\n",
    "\n",
    "        # VARIABLE: total trade volume\n",
    "        # Calculate the trading volume during the 10m\n",
    "        tv = rolling_window_data['trade_volume'].sum()\n",
    "        # Append n_trades to the list\n",
    "        trade_vol.append(tv)\n",
    "\n",
    "        # Assuming df is your DataFrame and 'A' is the column of interest\n",
    "        percentile_99 = rolling_window_data['trade_volume'].quantile(0.99)\n",
    "        top1p = rolling_window_data[rolling_window_data['trade_volume'] >= percentile_99]\n",
    "        tv_top1p = rolling_window_data[rolling_window_data['trade_volume'] >= percentile_99]['trade_volume'].sum()\n",
    "        tv_top1p_rate = tv_top1p/tv\n",
    "        trade_vol_top1p_ratio.append(tv_top1p_rate)\n",
    "\n",
    "        # VARIABLE: volume power\n",
    "        # Calculate the trading volume during the 10m\n",
    "        ask_sum = rolling_window_data[rolling_window_data['ask_bid'] == 'ASK']['trade_volume'].sum()\n",
    "        bid_sum = rolling_window_data[rolling_window_data['ask_bid'] == 'BID']['trade_volume'].sum()\n",
    "        vp = (bid_sum - ask_sum) / tv  # 후에 전체 거래량으로 나누어줄 예정이다.\n",
    "        volume_power.append(vp)\n",
    "\n",
    "        ask_sum_top1p = top1p[top1p['ask_bid'] == 'ASK']['trade_volume'].sum()\n",
    "        bid_sum_top1p = top1p[top1p['ask_bid'] == 'BID']['trade_volume'].sum()\n",
    "        vp_top1p = (bid_sum_top1p - ask_sum_top1p) / tv_top1p # 후에 전체 거래량으로 나누어줄 예정이다.\n",
    "        volume_power_top1p.append(vp_top1p)\n",
    "\n",
    "        ep = rolling_window_data['trade_price'].iloc[-1]\n",
    "        end_price.append(ep)\n",
    "\n",
    "        rts = rolling_window_data['trade_price'].iloc[-1]/previous_price\n",
    "        returns.append(rts)\n",
    "\n",
    "        # VARIABLE: prices_30s_for_NN\n",
    "        # rolling_window_data.set_index('datetime', inplace=True, drop = False)\n",
    "        # temp = rolling_window_data['log_return_from_pp'].resample('30S').last()\n",
    "        # prices_30s_for_NN.append(temp)\n",
    "\n",
    "    # Create a new DataFrame to store the results\n",
    "\n",
    "    # print(\"len(window_end):\",len(temp_index[10:]))\n",
    "    # print(\"len(window_start):\",len(temp_index[:-10]))\n",
    "    # print(\"len(realized_volatility_mean0):\",len(realized_volatility_mean0))\n",
    "    # print(\"len(realized_up_volatility_mean0):\",len(realized_up_volatility_mean0))\n",
    "    # print(\"len(realized_down_volatility_mean0):\",len(realized_down_volatility_mean0))\n",
    "    # print(\"len(num_trades):\",len(num_trades))\n",
    "    # print(\"len(lowest_return):\",len(lowest_return))\n",
    "    # print(\"len(highest_return):\",len(highest_return))\n",
    "    # print(\"len(high_low_gap):\",len(high_low_gap))\n",
    "    # print(\"len(trade_vol):\",len(trade_vol))\n",
    "    # print(\"len(trade_vol_top1p_ratio):\",len(trade_vol_top1p_ratio))\n",
    "    # print(\"len(volume_power):\",len(volume_power))\n",
    "    # print(\"len(volume_power_top1p):\",len(volume_power_top1p))\n",
    "    # print(\"len(end_price):\",len(end_price))\n",
    "    # print(\"len(returns):\",len(returns))\n",
    "\n",
    "    result_df = pd.DataFrame({\n",
    "        'window_start': window_start2,\n",
    "        'window_end': window_end2,\n",
    "        # 'realized_volatility': realized_volatility,\n",
    "        'realized_volatility_mean0':realized_volatility_mean0,\n",
    "        'realized_up_volatility_mean0':realized_up_volatility_mean0,\n",
    "        'realized_down_volatility_mean0':realized_down_volatility_mean0,\n",
    "        'num_trades': num_trades,\n",
    "        'lowest_return': lowest_return,\n",
    "        'highest_return': highest_return,\n",
    "        'high_low_gap': high_low_gap,\n",
    "        'trade_vol': trade_vol,\n",
    "        'trade_vol_top1p_ratio': trade_vol_top1p_ratio,\n",
    "        'volume_power': volume_power,\n",
    "        'volume_power_top1p': volume_power_top1p,\n",
    "        'end_price': end_price,\n",
    "        'returns': returns,\n",
    "        # 'prices_30s_for_NN': prices_30s_for_NN,\n",
    "    })\n",
    "\n",
    "    # Reset the index of the result DataFrame\n",
    "    result_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # 10분동안의 return을 안 만들었었네//\n",
    "    # result_df['return'] = np.log(result_df['end_price'].shift(-10) / result_df['end_price'])\n",
    "\n",
    "    result_df['return'] = result_df['returns']\n",
    "    result_df = result_df.drop('returns', axis=1)\n",
    "\n",
    "    # time_id 만들어주기.\n",
    "    result_df['window_start'] = pd.to_datetime(result_df['window_start'])\n",
    "    result_df['time_id'] = result_df['window_start'].dt.floor('1min').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    result_df['time_id'] = pd.to_datetime(result_df['time_id'])\n",
    "\n",
    "    # Bolinger Band \n",
    "    result_df['log_return_temp'] = result_df['end_price'].pct_change().apply(lambda x: np.log(1 + x))\n",
    "\n",
    "    result_df['BB_width_w20'] = result_df['log_return_temp'].rolling(20).std()*4\n",
    "    result_df['BB_width_w40'] = result_df['log_return_temp'].rolling(40).std()*4\n",
    "    result_df['BB_width_w10'] = result_df['log_return_temp'].rolling(10).std()*4\n",
    "\n",
    "    # 종속변수 만들기 (향후 10분의 realized vol / lowest return)\n",
    "    df_ticker_pd_1m = df_ticker_pd_1m.shift(1)\n",
    "    # result_df['dv1_realized_volatility'] = result_df['realized_volatility'].shift(-10)\n",
    "    result_df['dv5_realized_volatility_mean0'] = result_df['realized_volatility_mean0'].shift(-10)\n",
    "    result_df['dv2_lowest_return'] = result_df['lowest_return'].shift(-10)\n",
    "    result_df['dv3_highest_return'] = result_df['highest_return'].shift(-10)\n",
    "\n",
    "    ##### 450/300/150초 기준 변수 만들기 => 직전 1분. 직전 5분 으로 수정.\n",
    "\n",
    "    # Iterate through each time window using the rolling method\n",
    "\n",
    "    time_intervals = [60, 300]\n",
    "\n",
    "    for interval in time_intervals:\n",
    "\n",
    "        # realized_volatility = []\n",
    "        realized_volatility_mean0 = []\n",
    "        num_trades = []\n",
    "        lowest_return = []\n",
    "        highest_return = []\n",
    "        trade_vol = []\n",
    "        high_low_gap = []\n",
    "        volume_power = []\n",
    "        trade_vol_top1p_ratio = []\n",
    "        volume_power_top1p = []\n",
    "        window_start2 = []\n",
    "        window_end2 = []\n",
    "\n",
    "        print(\"###interval:\", interval)\n",
    "        for window_start in temp_index[:-10]:\n",
    "            window_end = window_start + window_size\n",
    "\n",
    "            if  (start_time <= window_start.time() <= end_time) or  (start_time <= window_end.time() <= end_time):\n",
    "                continue \n",
    "            \n",
    "            \n",
    "            # Filter the data for each rolling window\n",
    "            rolling_window_data = df_ticker_pd.loc[(df_ticker_pd['datetime'] >= window_end - pd.Timedelta(seconds=interval)) & (df_ticker_pd['datetime'] < window_end)]\n",
    "            # previous_price = rolling_window_data['trade_price'].iloc[0] # 10분 bin이 시작하기 직전 마지막 trade price 값을 의미.\n",
    "            \n",
    "            try: \n",
    "                previous_price = rolling_window_data['trade_price'].iloc[0]\n",
    "            except Exception as e:\n",
    "                print(\"No previous price.\")\n",
    "                continue\n",
    "\n",
    "            window_start2.append(window_start)\n",
    "            window_end2.append(window_end)\n",
    "\n",
    "            rolling_window_data.loc[:, 'log_return_from_pp'] = np.log(rolling_window_data['trade_price'] / previous_price)\n",
    "\n",
    "            # VARIABLE: realized_volatility \n",
    "            returns = rolling_window_data['log_return_from_pp']\n",
    "            # volatility = returns.std()\n",
    "            # realized_volatility.append(volatility)\n",
    "\n",
    "            squared_returns = np.square(returns)\n",
    "            average_squared_returns = np.mean(squared_returns)\n",
    "            realized_volatility_mean0.append(np.sqrt(average_squared_returns))\n",
    "\n",
    "            # VARIABLE: number of trades \n",
    "            n_trades = rolling_window_data.shape[0]\n",
    "            num_trades.append(n_trades)\n",
    "\n",
    "            # VARIABLE: lowest return\n",
    "            l_return = rolling_window_data['log_return_from_pp'].min()\n",
    "            lowest_return.append(l_return)\n",
    "\n",
    "            # VARIABLE: highest return\n",
    "            h_return = rolling_window_data['log_return_from_pp'].max()\n",
    "            highest_return.append(h_return) \n",
    "\n",
    "            # VARIABLE: high_low_gap\n",
    "            hl_gap = h_return - l_return\n",
    "            high_low_gap.append(hl_gap)\n",
    "\n",
    "            # VARIABLE: total trade volume\n",
    "            # Calculate the trading volume during the 10m\n",
    "            tv = rolling_window_data['trade_volume'].sum()\n",
    "            # Append n_trades to the list\n",
    "            trade_vol.append(tv)\n",
    "\n",
    "            # Assuming df is your DataFrame and 'A' is the column of interest\n",
    "            percentile_99 = rolling_window_data['trade_volume'].quantile(0.99)\n",
    "            top1p = rolling_window_data[rolling_window_data['trade_volume'] >= percentile_99]\n",
    "            tv_top1p = rolling_window_data[rolling_window_data['trade_volume'] >= percentile_99]['trade_volume'].sum()\n",
    "            tv_top1p_rate = tv_top1p/tv\n",
    "            trade_vol_top1p_ratio.append(tv_top1p_rate)\n",
    "\n",
    "            # VARIABLE: volume power\n",
    "            ask_sum = rolling_window_data[rolling_window_data['ask_bid'] == 'ASK']['trade_volume'].sum()\n",
    "            bid_sum = rolling_window_data[rolling_window_data['ask_bid'] == 'BID']['trade_volume'].sum()\n",
    "            vp = (bid_sum - ask_sum) / tv  \n",
    "            # if ask_sum >= bid_sum:\n",
    "            #     vp = -ask_sum/bid_sum\n",
    "            # elif bid_sum > ask_sum:\n",
    "            #     vp = bid_sum/ask_sum\n",
    "            volume_power.append(vp)\n",
    "\n",
    "            ask_sum_top1p = top1p[top1p['ask_bid'] == 'ASK']['trade_volume'].sum()\n",
    "            bid_sum_top1p = top1p[top1p['ask_bid'] == 'BID']['trade_volume'].sum()\n",
    "            vp_top1p = (bid_sum_top1p - ask_sum_top1p) / tv_top1p \n",
    "            volume_power_top1p.append(vp_top1p)    \n",
    "\n",
    "            ep = rolling_window_data['trade_price'].iloc[-1]\n",
    "            end_price.append(ep)\n",
    "\n",
    "        # Create a new DataFrame to store the results\n",
    "        d = pd.DataFrame({\n",
    "            f'window_start_{interval}': window_start2,\n",
    "            'window_end': window_end2,\n",
    "            # f'realized_volatility_{interval}': realized_volatility,\n",
    "            f'realized_volatility_mean0_{interval}': realized_volatility_mean0,\n",
    "            f'num_trades_{interval}': num_trades,\n",
    "            f'lowest_return_{interval}': lowest_return,\n",
    "            f'highest_return_{interval}': highest_return,\n",
    "            f'high_low_gap_{interval}': high_low_gap,\n",
    "            f'trade_vol_{interval}': trade_vol,\n",
    "            f'trade_vol_top1p_ratio_{interval}': trade_vol_top1p_ratio,\n",
    "            f'volume_power_{interval}': volume_power,\n",
    "            f'volume_power_top1p_{interval}': volume_power_top1p,\n",
    "        })\n",
    "        # print(\"###result_df:\", result_df.head(3))\n",
    "        # print(\"###result_df_{}:\".format(interval), result_df_150.head(3))\n",
    "        # result_df = pd.merge(result_df, result_df_150, on = 'window_start', how='inner')\n",
    "        # print(\"###merged result_df:\", result_df.head(3))\n",
    "        result_df = pd.merge(result_df, d, on = 'window_end', how='inner')\n",
    "\n",
    "    # agg = pd.merge(agg, d, on='time_id', how='left')  # 계산된 통계량을 기존 데이터프레임에 병합합니다.\n",
    "\n",
    "    # 첫 1분 및 마지막 11분은 데이터가 complete 하지 않을 것이므로 삭제.\n",
    "    start_time = result_df['time_id'].iloc[0]\n",
    "    end_time = result_df['time_id'].iloc[-11]\n",
    "    result_df_flt = result_df[(result_df['time_id'] > start_time) & (result_df['time_id'] < end_time)]\n",
    "\n",
    "    return result_df_flt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering (ORDERBOOK 부문)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7848044f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering_orderbook(df_orderbook):\n",
    "\n",
    "    # df_orderbook_pd2 = df_orderbook.compute() # dask dataframe => pandas dataframe\n",
    "    df_orderbook_pd2 = df_orderbook.copy()\n",
    "\n",
    "    gc.collect()\n",
    "    # for testing:\n",
    "    # df_orderbook_pd2 = df_orderbook_pd.iloc[:]\n",
    "\n",
    "    df_orderbook_pd2['datetime'] = pd.to_datetime(df_orderbook_pd2['datetime'])\n",
    "    # datetime을 1분 단위로 정리하는 변수 (dt_1m)을 만듬 (이는 데이터를 '정리'하는 데에 활용될 단위임)\n",
    "    df_orderbook_pd2['dt_1m'] = df_orderbook_pd2['datetime'].dt.floor('1min').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    df_orderbook_pd2['dt_1m'] = pd.to_datetime(df_orderbook_pd2['dt_1m'])\n",
    "    df_orderbook_pd2.set_index('dt_1m', inplace=True, drop = False)\n",
    "\n",
    "    # Define the size of the moving window (10 minutes in this case)\n",
    "    window_size = pd.Timedelta(minutes=10)\n",
    "    \n",
    "    # Define the time range\n",
    "    start_time = pd.to_datetime('00:00:00').time()\n",
    "    end_time = pd.to_datetime('06:00:00').time()\n",
    "\n",
    "    # Create an empty list to store features for each rolling window\n",
    "    liq_last_1 = []\n",
    "    liq_last_2 = []\n",
    "    liq_last_5 = []\n",
    "    # liq_last_10 = []\n",
    "    # liq_last_15 = []\n",
    "    ep_liq_1 =[]\n",
    "    ep_liq_2 =[]\n",
    "    ep_liq_5 =[]\n",
    "    # ep_liq_10 =[]\n",
    "    # ep_liq_15 =[]\n",
    "    bidask_spread_0 = []\n",
    "    bidask_spread_1 = []\n",
    "    prices_30s_for_NN = []\n",
    "    highest_possible_return = []\n",
    "    window_start2 = []\n",
    "    window_end2 = []\n",
    "    temp_index = df_orderbook_pd2.index.unique().tolist()\n",
    "    orderbook_bias_1 = []\n",
    "    orderbook_bias_2 = []\n",
    "    orderbook_bias_5 = []\n",
    "\n",
    "    # Iterate through each time window using the rolling method\n",
    "\n",
    "    print(\"df_orderbook_pd2.shape:\", df_orderbook_pd2.shape)\n",
    "    j = 0\n",
    "    print(\"feature expansion from orderbook #1\")\n",
    "    for window_start in temp_index[:-10]:\n",
    "        \n",
    "        window_end = window_start + window_size\n",
    "\n",
    "        if  (start_time <= window_start.time() <= end_time) or  (start_time <= window_end.time() <= end_time):\n",
    "            continue \n",
    "\n",
    "        # Check the progress\n",
    "        if j % 1000 ==0:\n",
    "            print(\"main iteration:\", j)\n",
    "\n",
    "        window_start2.append(window_start)\n",
    "        window_end2.append(window_end)\n",
    "        # print(\"window_start:\", window_start, \" & window_end:\", window_end)\n",
    "\n",
    "        # Filter the data for each rolling window\n",
    "        rolling_window_data = df_orderbook_pd2.loc[(df_orderbook_pd2.index >= window_start) & (df_orderbook_pd2.index < window_end)]\n",
    "        # previous_price = rolling_window_data['ending_price_b1m'].iloc[0] # 10분 bin이 시작하기 직전 마지막 trade price 값을 의미.\n",
    "        # rolling_window_data['log_return_from_pp'] = np.log(rolling_window_data['trade_price'] / previous_price)\n",
    "\n",
    "        first_order_book = rolling_window_data.iloc[0]\n",
    "        last_order_book = rolling_window_data.iloc[-1]\n",
    "\n",
    "        best_buying_price_at_start = first_order_book['orderbook_ap_0']\n",
    "        best_selling_price = rolling_window_data['orderbook_bp_0'].max()\n",
    "        highest_possible_return2 = best_selling_price / best_buying_price_at_start\n",
    "        highest_possible_return.append(highest_possible_return2) \n",
    "\n",
    "        # VARIABLE: liquidity measures\n",
    "        wap_1 = (last_order_book['orderbook_bp_0'] * last_order_book['orderbook_as_0'] + last_order_book['orderbook_ap_0'] * last_order_book['orderbook_bs_0'])/(last_order_book['orderbook_bs_0'] + last_order_book['orderbook_as_0'])\n",
    "        \n",
    "        # liq_1 = 0\n",
    "        # liq_2 = 0\n",
    "        # liq_5 = 0\n",
    "        # liq_10 = 0\n",
    "        # liq_15 = 0\n",
    "\n",
    "        # for i in range(1):\n",
    "        #     liq_1 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "        # for i in range(2):\n",
    "        #     liq_2 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "        # for i in range(5):\n",
    "        #     liq_5 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "        # for i in range(10):\n",
    "        #     liq_10 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "        data_liq = {}\n",
    "        temp = 0\n",
    "        for i in range(5):\n",
    "            temp += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "            if i+1 in [1, 2, 5]: # , 10, 15\n",
    "                # print(\"liquidity calculatino checker_{}\".format(i+1))\n",
    "                variable_name = f'liq_{i+1}'\n",
    "                data_liq[variable_name] = temp\n",
    "\n",
    "        # print(data_liq)\n",
    "        liq_last_1.append(data_liq['liq_1'])\n",
    "        liq_last_2.append(data_liq['liq_2'])\n",
    "        liq_last_5.append(data_liq['liq_5'])\n",
    "        # liq_last_10.append(data_liq['liq_10'])\n",
    "        # liq_last_15.append(data_liq['liq_15'])\n",
    "\n",
    "        ep_wap_1 = (rolling_window_data['orderbook_bp_0'].mean() * rolling_window_data['orderbook_as_0'].mean() + rolling_window_data['orderbook_ap_0'].mean() * rolling_window_data['orderbook_bs_0'].mean())/(rolling_window_data['orderbook_bs_0'].mean() + rolling_window_data['orderbook_as_0'].mean())\n",
    "\n",
    "        # entire_period_liq_1 = 0\n",
    "        # entire_period_liq_2 = 0\n",
    "        # entire_period_liq_5 = 0\n",
    "        # entire_period_liq_10 = 0\n",
    "        # entire_period_liq_15 = 0\n",
    "\n",
    "        # for i in range(1):\n",
    "        #     entire_period_liq_1 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "        # for i in range(2):\n",
    "        #     entire_period_liq_2 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "        # for i in range(5):\n",
    "        #     entire_period_liq_5 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "        # for i in range(10):\n",
    "        #     entire_period_liq_10 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "        data_ep_liq = {}\n",
    "        temp2 = 0\n",
    "        for i in range(5):\n",
    "            temp2 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "            if i+1 in [1, 2, 5]: # , 10, 15\n",
    "                variable_name = f'ep_liq_{i+1}'\n",
    "                data_ep_liq[variable_name] = temp2\n",
    "\n",
    "        ep_liq_1.append(data_ep_liq['ep_liq_1'])\n",
    "        ep_liq_2.append(data_ep_liq['ep_liq_2'])\n",
    "        ep_liq_5.append(data_ep_liq['ep_liq_5'])\n",
    "        # ep_liq_10.append(data_ep_liq['ep_liq_10'])\n",
    "        # ep_liq_15.append(data_ep_liq['ep_liq_15'])\n",
    "\n",
    "        # VARIABLE: bidask spread\n",
    "        ob_price_diff = []\n",
    "        for i in range(14):\n",
    "            ob_price_diff.append(last_order_book[f'orderbook_bp_{i}'] - last_order_book[f'orderbook_bp_{i+1}'])\n",
    "            ob_price_diff.append(last_order_book[f'orderbook_ap_{i+1}'] - last_order_book[f'orderbook_ap_{i}'])\n",
    "        tick_size = min(ob_price_diff)\n",
    "\n",
    "        # 마지막 1분 정보만 추출해서 bid ask spread의 평균을 구하자 (마지막 타이밍의 orderbook만 쓰는 것보다 낫지 않을까?)\n",
    "        last_1m = rolling_window_data.loc[(rolling_window_data.dt_1m == rolling_window_data['dt_1m'].iloc[-1])]\n",
    "\n",
    "        ba_sp_0 = (last_1m['orderbook_ap_0'] - last_1m['orderbook_bp_0'])/tick_size\n",
    "        bidask_spread_0.append(ba_sp_0.mean())\n",
    "        ba_sp_1 = (last_1m['orderbook_ap_1'] - last_1m['orderbook_bp_1'])/tick_size\n",
    "        bidask_spread_1.append(ba_sp_1.mean())\n",
    "        \n",
    "        ob_bias_1  = last_1m['orderbook_as_0'].sum() / last_1m['orderbook_bs_0'].sum() \n",
    "        ob_bias_2  = (last_1m['orderbook_as_0']+last_1m['orderbook_as_1']).sum() / (last_1m['orderbook_bs_0']+last_1m['orderbook_bs_1']).sum() \n",
    "        ob_bias_5  = (last_1m['orderbook_as_0']+last_1m['orderbook_as_1']+last_1m['orderbook_as_2']+last_1m['orderbook_as_3']+last_1m['orderbook_as_4']).sum()/ (last_1m['orderbook_bs_0']+last_1m['orderbook_bs_1']+last_1m['orderbook_bs_2']+last_1m['orderbook_bs_3']+last_1m['orderbook_bs_4']).sum()\n",
    "        orderbook_bias_1.append(ob_bias_1)\n",
    "        orderbook_bias_2.append(ob_bias_2)\n",
    "        orderbook_bias_5.append(ob_bias_5)\n",
    "\n",
    "        # liq_1 = [last_order_book['orderbook_bs_0']/(wap_1 - last_order_book['orderbook_bp_0']) + last_order_book['orderbook_as_0']/(last_order_book['orderbook_ap_0'] - wap_1)]\n",
    "        # liq_1 = liq_1 +  \n",
    "\n",
    "        # 30s prices 를 orderbook에서 가져와보자:\n",
    "        returns_per_30s_temp = []\n",
    "        distance = pd.Timedelta(seconds=30)\n",
    "        first_order_book = rolling_window_data.iloc[0]\n",
    "        first_wap = (first_order_book['orderbook_bp_0'] * first_order_book['orderbook_as_0'] + first_order_book['orderbook_ap_0'] * first_order_book['orderbook_bs_0'])/(first_order_book['orderbook_bs_0'] + first_order_book['orderbook_as_0'])\n",
    "        for i in range(20):\n",
    "            temp_30s_window_data = rolling_window_data.loc[(rolling_window_data.index >= window_start) & (rolling_window_data.index < window_start+distance*(i+1))]\n",
    "            temp_mid_order_book = temp_30s_window_data.iloc[-1]\n",
    "            mid_wap = (temp_mid_order_book['orderbook_bp_0'] * temp_mid_order_book['orderbook_as_0'] + temp_mid_order_book['orderbook_ap_0'] * temp_mid_order_book['orderbook_bs_0'])/(temp_mid_order_book['orderbook_bs_0'] + temp_mid_order_book['orderbook_as_0'])\n",
    "            temp_return = np.log(mid_wap/first_wap)\n",
    "            returns_per_30s_temp.append(temp_return)\n",
    "\n",
    "        prices_30s_for_NN.append(returns_per_30s_temp)\n",
    "        \n",
    "        j= j+1\n",
    "        gc.collect()\n",
    "\n",
    "    # Create a new DataFrame to store the results\n",
    "    result_df_orderbook = pd.DataFrame({\n",
    "        'window_start': window_start2,\n",
    "        'window_end': window_end2,\n",
    "        'liq_last_1': liq_last_1,\n",
    "        'liq_last_2': liq_last_2,\n",
    "        'liq_last_5': liq_last_5,\n",
    "        # 'liq_last_10': liq_last_10,\n",
    "        # 'liq_last_15': liq_last_15,\n",
    "        'ep_liq_1': ep_liq_1,\n",
    "        'ep_liq_2': ep_liq_2,\n",
    "        'ep_liq_5': ep_liq_5,\n",
    "        # 'ep_liq_10': ep_liq_10,\n",
    "        # 'ep_liq_15': ep_liq_15,\n",
    "        'bidask_spread_0': bidask_spread_0,\n",
    "        'bidask_spread_1': bidask_spread_1,\n",
    "        'prices_30s_for_NN': prices_30s_for_NN,\n",
    "        'highest_possible_return': highest_possible_return,\n",
    "        'orderbook_bias_1': orderbook_bias_1,\n",
    "        'orderbook_bias_2': orderbook_bias_2,\n",
    "        'orderbook_bias_5': orderbook_bias_5,\n",
    "    })\n",
    "\n",
    "    # Reset the index of the result DataFrame\n",
    "    result_df_orderbook.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # time_id 만들어주기.\n",
    "    result_df_orderbook['window_start'] = pd.to_datetime(result_df_orderbook['window_start'])\n",
    "    result_df_orderbook['time_id'] = result_df_orderbook['window_start'].dt.floor('1min').dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    result_df_orderbook['time_id'] = pd.to_datetime(result_df_orderbook['time_id'])\n",
    "\n",
    "    result_df_orderbook['dv6_highest_possible_return'] = result_df_orderbook['highest_possible_return'].shift(-10)\n",
    "\n",
    "    print(\"feature expansion from orderbook #2 (entire time processing done)\")\n",
    "\n",
    "\n",
    "    ###### To-do : 450/300/150초 기준 변수 만들기.\n",
    "\n",
    "    # Create an empty list to store features for each rolling window\n",
    "\n",
    "    time_intervals = [60, 300]\n",
    "\n",
    "    for interval in time_intervals:\n",
    "\n",
    "        print(\"feature expansion from orderbook #3 ({}s beginning)\".format(interval))\n",
    "\n",
    "        liq_last_1 = []\n",
    "        liq_last_2 = []\n",
    "        liq_last_5 = []\n",
    "        # liq_last_10 = []\n",
    "        # liq_last_15 = []\n",
    "        ep_liq_1 =[]\n",
    "        ep_liq_2 =[]\n",
    "        ep_liq_5 =[]\n",
    "        # ep_liq_10 =[]\n",
    "        # ep_liq_15 =[]        \n",
    "        # bidask_spread_0 = []\n",
    "        # bidask_spread_1 = []\n",
    "        window_start2 = []\n",
    "        window_end2 = []\n",
    "        j=0\n",
    "        for window_start in temp_index[:-10]:\n",
    "            window_end = window_start + window_size\n",
    "\n",
    "            if  (start_time <= window_start.time() <= end_time) or  (start_time <= window_end.time() <= end_time):\n",
    "                continue             \n",
    "\n",
    "            # Check the progress\n",
    "            if j % 1000 ==0:\n",
    "                print(\"subperiod_{} iteration:\".format(interval), j)\n",
    "\n",
    "            window_start2.append(window_start)\n",
    "            window_end2.append(window_end)\n",
    "            # Filter the data for each rolling window\n",
    "            rolling_window_data = df_orderbook_pd2.loc[(df_orderbook_pd2['datetime'] >= window_end - pd.Timedelta(seconds=interval)) & (df_orderbook_pd2['datetime'] < window_end )]\n",
    "\n",
    "            # VARIABLE: liquidity measures\n",
    "            last_order_book = rolling_window_data.iloc[-1]\n",
    "            wap_1 = (last_order_book['orderbook_bp_0'] * last_order_book['orderbook_as_0'] + last_order_book['orderbook_ap_0'] * last_order_book['orderbook_bs_0'])/(last_order_book['orderbook_bs_0'] + last_order_book['orderbook_as_0'])\n",
    "            \n",
    "            # liq_1 = 0\n",
    "            # for i in range(1):\n",
    "            #     liq_1 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "            # liq_2 = 0\n",
    "            # for i in range(2):\n",
    "            #     liq_2 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "            # liq_5 = 0\n",
    "            # for i in range(5):\n",
    "            #     liq_5 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "            # liq_10 = 0\n",
    "            # for i in range(10):\n",
    "            #     liq_10 += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "\n",
    "            # liq_15 = 0\n",
    "            data_liq = {}\n",
    "            temp = 0\n",
    "            for i in range(5):\n",
    "                temp += last_order_book[f'orderbook_bs_{i}']/(wap_1 - last_order_book[f'orderbook_bp_{i}']) + last_order_book[f'orderbook_as_{i}']/(last_order_book[f'orderbook_ap_{i}'] - wap_1)\n",
    "                if i+1 in [1, 2, 5]: #, 10, 15\n",
    "                    variable_name = f'liq_{i+1}'\n",
    "                    data_liq[variable_name] = temp\n",
    "\n",
    "\n",
    "            liq_last_1.append(data_liq['liq_1'])\n",
    "            liq_last_2.append(data_liq['liq_2'])\n",
    "            liq_last_5.append(data_liq['liq_5'])\n",
    "            # liq_last_10.append(data_liq['liq_10'])\n",
    "            # liq_last_15.append(data_liq['liq_15'])\n",
    "\n",
    "            ep_wap_1 = (rolling_window_data['orderbook_bp_0'].mean() * rolling_window_data['orderbook_as_0'].mean() + rolling_window_data['orderbook_ap_0'].mean() * rolling_window_data['orderbook_bs_0'].mean())/(rolling_window_data['orderbook_bs_0'].mean() + rolling_window_data['orderbook_as_0'].mean())\n",
    "\n",
    "            # entire_period_liq_1 = 0\n",
    "            # for i in range(1):\n",
    "            #     entire_period_liq_1 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "            # entire_period_liq_2 = 0\n",
    "            # for i in range(2):\n",
    "            #     entire_period_liq_2 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "            # entire_period_liq_5 = 0\n",
    "            # for i in range(5):\n",
    "            #     entire_period_liq_5 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "            # entire_period_liq_10 = 0\n",
    "            # for i in range(10):\n",
    "            #     entire_period_liq_10 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "\n",
    "            data_ep_liq = {}\n",
    "            temp2 = 0\n",
    "            # entire_period_liq_15 = 0\n",
    "            for i in range(5):\n",
    "                temp2 += rolling_window_data[f'orderbook_bs_{i}'].mean()/(ep_wap_1 - rolling_window_data[f'orderbook_bp_{i}'].mean()) + rolling_window_data[f'orderbook_as_{i}'].mean()/(rolling_window_data[f'orderbook_ap_{i}'].mean() - ep_wap_1)\n",
    "                if i+1 in [1,2, 5]: # , 10, 15\n",
    "                    variable_name = f'ep_liq_{i+1}'\n",
    "                    data_ep_liq[variable_name]=temp2\n",
    "            ep_liq_1.append(data_ep_liq['ep_liq_1'])\n",
    "            ep_liq_2.append(data_ep_liq['ep_liq_2'])\n",
    "            ep_liq_5.append(data_ep_liq['ep_liq_5'])\n",
    "            # ep_liq_10.append(data_ep_liq['ep_liq_10'])\n",
    "            # ep_liq_15.append(data_ep_liq['ep_liq_15'])\n",
    "\n",
    "            # # VARIABLE: bidask spread\n",
    "            # ob_price_diff = []\n",
    "            # for i in range(14):\n",
    "            #     ob_price_diff.append(last_order_book[f'orderbook_bp_{i}'] - last_order_book[f'orderbook_bp_{i+1}'])\n",
    "            #     ob_price_diff.append(last_order_book[f'orderbook_ap_{i+1}'] - last_order_book[f'orderbook_ap_{i}'])\n",
    "            # tick_size = min(ob_price_diff)\n",
    "\n",
    "            # # 마지막 1분 정보만 추출해서 bid ask spread의 평균을 구하자 (마지막 타이밍의 orderbook만 쓰는 것보다 낫지 않을까?)\n",
    "            # last_1m = rolling_window_data.loc[(rolling_window_data.dt_1m == rolling_window_data['dt_1m'].iloc[-1])]\n",
    "\n",
    "            # ba_sp_0 = (last_1m['orderbook_ap_0'] - last_1m['orderbook_bp_0'])/tick_size\n",
    "            # bidask_spread_0.append(ba_sp_0.mean())\n",
    "            # ba_sp_1 = (last_1m['orderbook_ap_1'] - last_1m['orderbook_bp_1'])/tick_size\n",
    "            # bidask_spread_1.append(ba_sp_1.mean())\n",
    "            \n",
    "            j = j+1\n",
    "            gc.collect()\n",
    "\n",
    "        # Create a new DataFrame to store the results\n",
    "        d = pd.DataFrame({\n",
    "        f'window_start_{interval}': window_start2,\n",
    "        'window_end': window_end2,\n",
    "        f'liq_last_1_{interval}': liq_last_1,\n",
    "        f'liq_last_2_{interval}': liq_last_2,\n",
    "        f'liq_last_5_{interval}': liq_last_5,\n",
    "        # f'liq_last_10_{interval}': liq_last_10,\n",
    "        # f'liq_last_15_{interval}': liq_last_15,\n",
    "        f'ep_liq_1_{interval}': ep_liq_1,\n",
    "        f'ep_liq_2_{interval}': ep_liq_2,\n",
    "        f'ep_liq_5_{interval}': ep_liq_5,\n",
    "        # f'ep_liq_10_{interval}': ep_liq_10,\n",
    "        # f'ep_liq_15_{interval}': ep_liq_15,\n",
    "        # f'bidask_spread_0_{interval}': bidask_spread_0,\n",
    "        # f'bidask_spread_1_{interval}': bidask_spread_1,\n",
    "        })\n",
    "\n",
    "        result_df_orderbook = pd.merge(result_df_orderbook, d, on = 'window_end', how='inner')\n",
    "        gc.collect()\n",
    "\n",
    "    # 첫 1분 및 마지막 11분은 데이터가 complete 하지 않을 것이므로 삭제.\n",
    "    start_time = result_df_orderbook['time_id'].iloc[0]\n",
    "    end_time = result_df_orderbook['time_id'].iloc[-11]\n",
    "    result_df_orderbook_flt = result_df_orderbook[(result_df_orderbook['time_id'] > start_time) & (result_df_orderbook['time_id'] < end_time)]\n",
    "\n",
    "    return result_df_orderbook_flt\n",
    "\n",
    "    # # Orderbook 10분 정리 dataframe을 중간 저장.\n",
    "    # result_df_orderbook_flt.to_csv(working_directory + \"output\\\\{}_sum_orderbook_10m.csv\".format(coin), index=False)\n",
    "\n",
    "    # ## result_df & result_df_orderbook 두개를 concat 하자.\n",
    "    # result_df_orderbook_flt = result_df_orderbook_flt.drop(columns = ['window_end', 'time_id'])\n",
    "    # combined_result_df = pd.merge(result_df_flt, result_df_orderbook_flt, on='window_start', suffixes=('_ticker', '_orderbook'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0df245ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hhkim\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###interval: 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hhkim\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\core\\indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous price.\n",
      "No previous price.\n",
      "No previous price.\n",
      "No previous price.\n",
      "No previous price.\n",
      "No previous price.\n",
      "No previous price.\n",
      "No previous price.\n",
      "No previous price.\n",
      "No previous price.\n",
      "No previous price.\n",
      "###interval: 300\n",
      "df_orderbook_pd2.shape: (10323127, 100)\n",
      "feature expansion from orderbook #1\n",
      "main iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hhkim\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:102: RuntimeWarning: divide by zero encountered in float_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main iteration: 1000\n",
      "main iteration: 2000\n",
      "main iteration: 3000\n",
      "main iteration: 4000\n",
      "main iteration: 5000\n",
      "main iteration: 6000\n",
      "main iteration: 7000\n",
      "main iteration: 8000\n",
      "main iteration: 9000\n",
      "main iteration: 10000\n",
      "main iteration: 11000\n",
      "main iteration: 12000\n",
      "main iteration: 13000\n",
      "main iteration: 14000\n",
      "main iteration: 15000\n",
      "main iteration: 16000\n",
      "main iteration: 17000\n",
      "main iteration: 18000\n",
      "main iteration: 19000\n",
      "main iteration: 20000\n",
      "main iteration: 21000\n",
      "main iteration: 22000\n",
      "main iteration: 23000\n",
      "main iteration: 24000\n",
      "main iteration: 25000\n",
      "main iteration: 26000\n",
      "main iteration: 27000\n",
      "main iteration: 28000\n",
      "main iteration: 29000\n",
      "main iteration: 30000\n",
      "main iteration: 31000\n",
      "main iteration: 32000\n",
      "main iteration: 33000\n",
      "main iteration: 34000\n",
      "main iteration: 35000\n",
      "main iteration: 36000\n",
      "main iteration: 37000\n",
      "main iteration: 38000\n",
      "main iteration: 39000\n",
      "main iteration: 40000\n",
      "main iteration: 41000\n",
      "main iteration: 42000\n",
      "main iteration: 43000\n",
      "main iteration: 44000\n",
      "main iteration: 45000\n",
      "main iteration: 46000\n",
      "main iteration: 47000\n",
      "main iteration: 48000\n",
      "main iteration: 49000\n",
      "main iteration: 50000\n",
      "main iteration: 51000\n",
      "main iteration: 52000\n",
      "main iteration: 53000\n",
      "main iteration: 54000\n",
      "main iteration: 55000\n",
      "main iteration: 56000\n",
      "main iteration: 57000\n",
      "main iteration: 58000\n",
      "main iteration: 59000\n",
      "main iteration: 60000\n",
      "main iteration: 61000\n",
      "main iteration: 62000\n",
      "main iteration: 63000\n",
      "main iteration: 64000\n",
      "main iteration: 65000\n",
      "main iteration: 66000\n",
      "main iteration: 67000\n",
      "main iteration: 68000\n",
      "main iteration: 69000\n",
      "main iteration: 70000\n",
      "main iteration: 71000\n",
      "main iteration: 72000\n",
      "main iteration: 73000\n",
      "main iteration: 74000\n",
      "main iteration: 75000\n",
      "main iteration: 76000\n",
      "feature expansion from orderbook #2 (entire time processing done)\n",
      "feature expansion from orderbook #3 (60s beginning)\n",
      "subperiod_60 iteration: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hhkim\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\ipykernel_launcher.py:291: RuntimeWarning: divide by zero encountered in float_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subperiod_60 iteration: 1000\n",
      "subperiod_60 iteration: 2000\n",
      "subperiod_60 iteration: 3000\n",
      "subperiod_60 iteration: 4000\n",
      "subperiod_60 iteration: 5000\n",
      "subperiod_60 iteration: 6000\n",
      "subperiod_60 iteration: 7000\n",
      "subperiod_60 iteration: 8000\n",
      "subperiod_60 iteration: 9000\n",
      "subperiod_60 iteration: 10000\n",
      "subperiod_60 iteration: 11000\n",
      "subperiod_60 iteration: 12000\n",
      "subperiod_60 iteration: 13000\n",
      "subperiod_60 iteration: 14000\n",
      "subperiod_60 iteration: 15000\n",
      "subperiod_60 iteration: 16000\n",
      "subperiod_60 iteration: 17000\n",
      "subperiod_60 iteration: 18000\n",
      "subperiod_60 iteration: 19000\n",
      "subperiod_60 iteration: 20000\n",
      "subperiod_60 iteration: 21000\n",
      "subperiod_60 iteration: 22000\n",
      "subperiod_60 iteration: 23000\n",
      "subperiod_60 iteration: 24000\n",
      "subperiod_60 iteration: 25000\n",
      "subperiod_60 iteration: 26000\n",
      "subperiod_60 iteration: 27000\n",
      "subperiod_60 iteration: 28000\n",
      "subperiod_60 iteration: 29000\n",
      "subperiod_60 iteration: 30000\n",
      "subperiod_60 iteration: 31000\n",
      "subperiod_60 iteration: 32000\n",
      "subperiod_60 iteration: 33000\n",
      "subperiod_60 iteration: 34000\n",
      "subperiod_60 iteration: 35000\n",
      "subperiod_60 iteration: 36000\n",
      "subperiod_60 iteration: 37000\n",
      "subperiod_60 iteration: 38000\n",
      "subperiod_60 iteration: 39000\n",
      "subperiod_60 iteration: 40000\n",
      "subperiod_60 iteration: 41000\n",
      "subperiod_60 iteration: 42000\n",
      "subperiod_60 iteration: 43000\n",
      "subperiod_60 iteration: 44000\n",
      "subperiod_60 iteration: 45000\n",
      "subperiod_60 iteration: 46000\n",
      "subperiod_60 iteration: 47000\n",
      "subperiod_60 iteration: 48000\n",
      "subperiod_60 iteration: 49000\n",
      "subperiod_60 iteration: 50000\n",
      "subperiod_60 iteration: 51000\n",
      "subperiod_60 iteration: 52000\n",
      "subperiod_60 iteration: 53000\n",
      "subperiod_60 iteration: 54000\n",
      "subperiod_60 iteration: 55000\n",
      "subperiod_60 iteration: 56000\n",
      "subperiod_60 iteration: 57000\n",
      "subperiod_60 iteration: 58000\n",
      "subperiod_60 iteration: 59000\n",
      "subperiod_60 iteration: 60000\n",
      "subperiod_60 iteration: 61000\n",
      "subperiod_60 iteration: 62000\n",
      "subperiod_60 iteration: 63000\n",
      "subperiod_60 iteration: 64000\n",
      "subperiod_60 iteration: 65000\n",
      "subperiod_60 iteration: 66000\n",
      "subperiod_60 iteration: 67000\n",
      "subperiod_60 iteration: 68000\n",
      "subperiod_60 iteration: 69000\n",
      "subperiod_60 iteration: 70000\n",
      "subperiod_60 iteration: 71000\n",
      "subperiod_60 iteration: 72000\n",
      "subperiod_60 iteration: 73000\n",
      "subperiod_60 iteration: 74000\n",
      "subperiod_60 iteration: 75000\n",
      "subperiod_60 iteration: 76000\n",
      "feature expansion from orderbook #3 (300s beginning)\n",
      "subperiod_300 iteration: 0\n",
      "subperiod_300 iteration: 1000\n",
      "subperiod_300 iteration: 2000\n",
      "subperiod_300 iteration: 3000\n",
      "subperiod_300 iteration: 4000\n",
      "subperiod_300 iteration: 5000\n",
      "subperiod_300 iteration: 6000\n",
      "subperiod_300 iteration: 7000\n",
      "subperiod_300 iteration: 8000\n",
      "subperiod_300 iteration: 9000\n",
      "subperiod_300 iteration: 10000\n",
      "subperiod_300 iteration: 11000\n",
      "subperiod_300 iteration: 12000\n",
      "subperiod_300 iteration: 13000\n",
      "subperiod_300 iteration: 14000\n",
      "subperiod_300 iteration: 15000\n",
      "subperiod_300 iteration: 16000\n",
      "subperiod_300 iteration: 17000\n",
      "subperiod_300 iteration: 18000\n",
      "subperiod_300 iteration: 19000\n",
      "subperiod_300 iteration: 20000\n",
      "subperiod_300 iteration: 21000\n",
      "subperiod_300 iteration: 22000\n",
      "subperiod_300 iteration: 23000\n",
      "subperiod_300 iteration: 24000\n",
      "subperiod_300 iteration: 25000\n",
      "subperiod_300 iteration: 26000\n",
      "subperiod_300 iteration: 27000\n",
      "subperiod_300 iteration: 28000\n",
      "subperiod_300 iteration: 29000\n",
      "subperiod_300 iteration: 30000\n",
      "subperiod_300 iteration: 31000\n",
      "subperiod_300 iteration: 32000\n",
      "subperiod_300 iteration: 33000\n",
      "subperiod_300 iteration: 34000\n",
      "subperiod_300 iteration: 35000\n",
      "subperiod_300 iteration: 36000\n",
      "subperiod_300 iteration: 37000\n",
      "subperiod_300 iteration: 38000\n",
      "subperiod_300 iteration: 39000\n",
      "subperiod_300 iteration: 40000\n",
      "subperiod_300 iteration: 41000\n",
      "subperiod_300 iteration: 42000\n",
      "subperiod_300 iteration: 43000\n",
      "subperiod_300 iteration: 44000\n",
      "subperiod_300 iteration: 45000\n",
      "subperiod_300 iteration: 46000\n",
      "subperiod_300 iteration: 47000\n",
      "subperiod_300 iteration: 48000\n",
      "subperiod_300 iteration: 49000\n",
      "subperiod_300 iteration: 50000\n",
      "subperiod_300 iteration: 51000\n",
      "subperiod_300 iteration: 52000\n",
      "subperiod_300 iteration: 53000\n",
      "subperiod_300 iteration: 54000\n",
      "subperiod_300 iteration: 55000\n",
      "subperiod_300 iteration: 56000\n",
      "subperiod_300 iteration: 57000\n",
      "subperiod_300 iteration: 58000\n",
      "subperiod_300 iteration: 59000\n",
      "subperiod_300 iteration: 60000\n",
      "subperiod_300 iteration: 61000\n",
      "subperiod_300 iteration: 62000\n",
      "subperiod_300 iteration: 63000\n",
      "subperiod_300 iteration: 64000\n",
      "subperiod_300 iteration: 65000\n",
      "subperiod_300 iteration: 66000\n",
      "subperiod_300 iteration: 67000\n",
      "subperiod_300 iteration: 68000\n",
      "subperiod_300 iteration: 69000\n",
      "subperiod_300 iteration: 70000\n",
      "subperiod_300 iteration: 71000\n",
      "subperiod_300 iteration: 72000\n",
      "subperiod_300 iteration: 73000\n",
      "subperiod_300 iteration: 74000\n",
      "subperiod_300 iteration: 75000\n",
      "subperiod_300 iteration: 76000\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "vn = 8 ### 같은 데이터셋 안에서도 vn이 다를 수 있음.. (feature의 변화라던지..) \n",
    "\n",
    "df = df.compute() ############################################# dask dataframe 이면 pandas dataframe으로 바꾸어주어야.\n",
    "\n",
    "# Convert float64 columns to float32\n",
    "float_columns = df.select_dtypes(include=['float64'])\n",
    "df = df.assign(**{col: df[col].astype('float32') for col in float_columns.columns})\n",
    "int64_columns = df.select_dtypes(include=['int64'])\n",
    "df = df.assign(**{col: df[col].astype('int32') for col in int64_columns.columns})\n",
    "\n",
    "df_orderbook = df[df['type_websocket']==\"orderbook\"]\n",
    "df_ticker = df[df['type_websocket']==\"ticker\"]\n",
    "\n",
    "df_ticker = df_ticker.set_index('sys_datetime', drop=False)\n",
    "df_orderbook = df_orderbook.set_index('sys_datetime', drop=False)\n",
    "\n",
    "gc.collect()\n",
    "# df_features_ticker = pd.read_csv(working_directory+\"output\\\\BTC_sum_ticker_10m_202303271051_v7.csv\")\n",
    "df_features_ticker = feature_engineering_ticker(df_ticker)\n",
    "# df_features_ticker['window_start'] = pd.to_datetime(df_features_ticker['window_start'])\n",
    "df_features_ticker['window_end'] = pd.to_datetime(df_features_ticker['window_end'])\n",
    "df_features_ticker.to_csv(working_directory + \"output\\\\{}_sum_ticker_10m_{}_v{}.csv\".format(coin, data_id, vn), index=False)\n",
    "\n",
    "df_features_orderbook = feature_engineering_orderbook(df_orderbook)\n",
    "df_features_orderbook.to_csv(working_directory + \"output\\\\{}_sum_orderbook_10m_{}_v{}.csv\".format(coin, data_id, vn), index=False)\n",
    "# df_features_orderbook = pd.read_csv(\"output\\\\BTC_sum_orderbook_10m_202312122122_v7.csv\")\n",
    "\n",
    "# df_features_orderbook = df_features_orderbook.drop(columns = ['window_end', 'time_id'])\n",
    "df_features_orderbook = df_features_orderbook.drop(columns = ['window_start', 'time_id'])\n",
    "\n",
    "# df_features_orderbook['window_start'] = pd.to_datetime(df_features_orderbook['window_start'])\n",
    "df_features_orderbook['window_end'] = pd.to_datetime(df_features_orderbook['window_end'])\n",
    "\n",
    "combined_result_df = pd.merge(df_features_ticker, df_features_orderbook, on='window_end', suffixes=('_ticker', '_orderbook'))\n",
    "\n",
    "## 추가변수 만들기\n",
    "# combined_result_df['realized_volatility_30s'] = np.nan\n",
    "# combined_result_df['dv4_realized_volatility_30s'] = np.nan\n",
    "combined_result_df['prices_30s_for_NN_onlyprices'] = np.nan\n",
    "combined_result_df['prices_30s_for_NN_onlyprices'] = combined_result_df['prices_30s_for_NN_onlyprices'].astype(str)\n",
    "\n",
    "num_rows = combined_result_df.shape[0]\n",
    "for i in range(num_rows):\n",
    "    temp_series = combined_result_df['prices_30s_for_NN'].iloc[i]\n",
    "    # # Split the string into a list based on the \"\\n\" marker\n",
    "    # tokenized_list = temp_series.split(\"\\n\")\n",
    "    # # Remove any leading or trailing whitespace from each element\n",
    "    # tokenized_list = [item.strip() for item in tokenized_list]\n",
    "    # # Assuming you already have a list named \"numbers\"\n",
    "    # if tokenized_list:\n",
    "    #     tokenized_list.pop(0)  # This will remove the first item in the list\n",
    "    #     tokenized_list.pop(-1)  # This will remove the first item in the list\n",
    "    # # Extract the return numbers using list comprehension\n",
    "    # return_numbers = [float(item.split()[-1]) for item in tokenized_list]\n",
    "    # return_numbers = temp_series\n",
    "    # temp_len = len(return_numbers)\n",
    "    # squared_numbers = [x ** 2 for x in return_numbers]\n",
    "    # sum_of_squared_numbers = sum(squared_numbers)\n",
    "    # mean_of_sum_of_squared_numbers = sum_of_squared_numbers/temp_len\n",
    "    # temp_vol = mean_of_sum_of_squared_numbers ** 0.5\n",
    "    # combined_result_df.at[i, 'realized_volatility_30s'] = temp_vol\n",
    "    return_numbers_str = ' '.join(str(num) for num in temp_series)\n",
    "    combined_result_df.at[i, 'prices_30s_for_NN_onlyprices'] = return_numbers_str\n",
    "    gc.collect()\n",
    "\n",
    "# combined_result_df['dv4_realized_volatility_30s'] = combined_result_df['realized_volatility_30s'].shift(-10)\n",
    "\n",
    "## Additional feature engineering (ex: TVLQ)\n",
    "combined_result_df['trade.tau'] = np.sqrt(1 / combined_result_df['num_trades'])\n",
    "\n",
    "combined_result_df['tvpl1'] = combined_result_df['trade_vol'] / combined_result_df['liq_last_1']\n",
    "combined_result_df['tvpl2'] = combined_result_df['trade_vol'] / combined_result_df['liq_last_2'] \n",
    "combined_result_df['tvpl5'] = combined_result_df['trade_vol'] / combined_result_df['liq_last_5'] \n",
    "# combined_result_df['tvpl10'] = combined_result_df['trade_vol'] / combined_result_df['liq_last_10'] \n",
    "# combined_result_df['tvpl15'] = combined_result_df['trade_vol'] / combined_result_df['liq_last_15']\n",
    "\n",
    "combined_result_df['tvpl_ep1'] = combined_result_df['trade_vol'] / combined_result_df['ep_liq_1']\n",
    "combined_result_df['tvpl_ep2'] = combined_result_df['trade_vol'] / combined_result_df['ep_liq_2']\n",
    "combined_result_df['tvpl_ep5'] = combined_result_df['trade_vol'] / combined_result_df['ep_liq_5']\n",
    "# combined_result_df['tvpl_ep10'] = combined_result_df['trade_vol'] / combined_result_df['ep_liq_10']\n",
    "# combined_result_df['tvpl_ep15'] = combined_result_df['trade_vol'] / combined_result_df['ep_liq_15']\n",
    "\n",
    "# 아래 작업시 빠른 작업을 위해 10분씩 정리된 dataframe을 중간 저장.\n",
    "combined_result_df.to_csv(working_directory + \"output\\\\{}_sum_both_10m_{}_v{}.csv\".format(coin, data_id, vn), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057da4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
